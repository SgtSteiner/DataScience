{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCCION A LAS REDES NEURONALES ARTIFICIALES CON KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pájaros nos inspiraron para volar, las plantas de bardana inspiraron el velcro e innumerables inventos más se inspiraron en la naturaleza. Parece lógico, entonces, mirar la arquitectura del cerebro en busca de inspiración sobre cómo construir una máquina inteligente. Esta es la idea clave que sirvió de chispa para las *redes neuronales artificiales - RNAs (artificial neuronal networks - ANN)*. Sin embargo, aunque los aviones se inspiraron en los pájaros, no tienen que batir sus alas. De forma similiar, las RNAs se han vuelto gradualmente bastante diferentes de sus primas biológicas. Algunos investigadores incluso argumentan que deberíamos abandonar por completo la analogía biológica (por ejemplo, diciendo \"unidades\" en lugar de \"neuronas\"), para que no restrinjamos nuestra creatividad a sistemas biológicamente plausibles (*podemos obtener lo mejor de ambos mundos estando abiertos a inspiraciones biológicas sin temor a crear modelos biológicamente irreales, siempre que funcionen bien*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las RNAs son el corazón del Deep Learning. Son versátiles, poderosas y escalables, haciéndolas ideales para abordar tareas grandes y altamente complejas de machine learning, tales como la clasificación de billones de imágenes (por ejemplo, Google Images), potenciar los servicios de reconocimiento del habla (por ejemplo, Siri de Apple), recomendar los mejores videos para ver a cientos de millones de usuarios cada dia (por ejemplo, YouTube) o aprendiendo a vencer al campeón del mundo del juego de *Go* jugando millones de partidas contra sí misma (Alpha-Zero de DeepMind)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la primera parte de este capítulo, nos introduciremos en las redes neuronales artificiales, empezando por un rápido recorrido por las primera arquitecturas de RNAs, hasta llegar a los *Perceptrones Multi-Capa PMCs* (*Multi-Layer Perceptrons, MLPs*), que se utilizan mucho en la actualidad (en otros capítulos exploraremos otras arquitecturas). En la segunda parte, echaremos un vistazo a cómo implementar RNAs usando la popular API Keras. Se trata de una API de alto nivel maravillosamente diseñada para la construcción, entrenamiento, evaluación y ejecución de redes neuronales. Pero no nos dejemos engañar por su simplicidad: es lo suficientemente expresiva y flexible para construir una amplia variedad de arquitecturas de redes neuronales. De hecho, probablemente será suficiente para la mayoría de nuestros casos. Además, si alguna vez necesitamos flexibilidad extra, siempre podremos escribir componentes de Keras personalizados usando su API de bajo nivel, como veremos más adelante.\n",
    "\n",
    "Pero primero, retrocedamos en el tiempo para ver cómo surgieron las redes neuronales artificiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De las neuronas biológicas a las artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorprendentemente, las RNAs existen desde hace bastante poco: fueron introducidas por primera vez en 1943 por el neurofisiólogo Warren McCulloch y el matemático Walter Pitts. En su [Artículo](https://homl.info/43) “A Logical Calculus of Ideas Immanent in Nervous Activity”, McCulloh y Pitts presentaron un modelo computacional simplificado de cómo las neuronas biológicas podrían trabajar juntas en el cerebro de los animales para ejecutar cálculos complejos usando *lógica proposicional*. Esta fue la primera arquitectura de red neuronal artifical. Desde entonces, se han inventado otras muchas arquitecturas, como veremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los primeros éxitos de las RNAs hasta la década de 1960 llevaron a la creencia generalizada de que pronto estaríamos conversando con máquinas verdaderamente inteligentes. Cuando quedó claro que esa promesa no sería cumplida (al menos durante bastante tiempo), los fondos y la inversión volaron hacia otra parte y las RNAs entraron en un largo invierno. A principios de la década de 1980 se produjo un resurgimiento del interés en el *conexionismo* (el estudio de las redes neuronales), a medida que se inventaron nuevas arquitecturas y se desarrollaron mejores técnicas de entrenamiento. Pero el progreso fue lento y en la década de 1990 se inventaron otras poderosas técnicas de Machine Learning, tales como las Máquinas de Soporte Vectorial. Estas técnicas parecían ofrecer mejores resultados y bases teóricas más solidas que las RNAs, así que una vez más el estudio de las redes neuronales entró en hibernación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, ahora estamos presenciando otra ola de interés en las RNAs. ¿Esta ola se extinguirá como las anteriores? Bueno, existen buenas razones para creer que esta ola es diferente y que tendrá un impacto mucho más profundo en nuestras vidas:\n",
    "\n",
    "+ Ahora existe una enorme cantidad de datos disponibles para entrenar a las redes neuronales y las RNAs superan frecuentemente a otras técnicas de ML en problemas muy grandes y complejos.\n",
    "\n",
    "+ El tremendo incremento en poder computacional desde la década de 1990 posibilita ahora entrenar grandes redes neuronales en una cantidad de tiempo razonable. Esto es en parte debido a la Ley de Moore, pero también gracias a la industria del juego, que ha producido por millones poderosas tarjetas GPU.\n",
    "\n",
    "+ Los algoritmos de entrenamiento han sido mejorados. Para ser justos, solo son ligeramente diferentes de los usados en la década de 1990, pero estos pequeños ajustes han tenido un impacto enorme.\n",
    "\n",
    "+ Algunas limitaciones teóricas de las RNAs han resultado ser benignas en la práctica. Por ejemplo, mucha gente pensó que los algoritmos de entrenamiento estaban condenados porque era probable que se atascaran en óptimos locales, pero resulta que esto es bastante raro en la práctica (o cuando es el caso, generalmente están bastante cerca del óptimo global).\n",
    "\n",
    "+ Las RNAs parecen haber entrado en un círculo virtuoso de financiación y progreso. Asombrosos productos basados en RNAs aparecen regularmente en las portadas de las noticias, lo que atrae cada vez más la atención y financiación hacia ellos, resultando en más y más progreso e incluso en más productos asombrosos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronas biológicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de que discutamos las neuronas artificiales, echemos un rápido vistazo a las neuronas biológicas (representadas en la siguiente figura). Es una célula de aspecto inusual que se encuentra en la corteza cerebral animal (por ejemplo, nuestro cerebro), compuesta por un *cuerpo celular*, conteniendo el núcleo y la mayoría de los componentes complejos de la célula, y muchas extensiones ramificadas denominadas *dendritas*, más una larga extensión llamado *axón*. La longitud del axón puede ser solo un poco más largo que el cuerpo de la célula o hasta decenas de miles de veces más largo. Cerca de su extremidad, el axón se divide en muchas ramas llamadas *telodendritas* y en la punta de esas ramificaciones hay minúsculas estructuras llamadas *terminales sinápticas* (o simplemente *sinapsis*), que están conectadas a las dendritas (o directamente al cuerpo de la célula) de otras neuronas. Las neuronas biológicas reciben impulsos eléctricos cortos llamados *señales* de otras neuronas a través de estas sinapsis. Cuando una neurona recibe un número suficiente de señales de otras neuronas en unos pocos milisegundos, dispara sus propias señales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biological_neuron](images/ch10/biological_neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, las neuronas biológicas individuales parecen comportarse de una manera bastante simple, pero están organizadas en un vasta red de billones de neuronas, cada neurona conectada normalmente a miles de otras neuronas. Se pueden realizar cálculos altamente complejos mediante una red de neuronas bastante simples, al igual que un hormiguero puede emerger de los esfuerzos combinados de simples hormigas. La arquitectura de las redes neuronales biológicas (RNB) es todavía objeto de investigación activa, pero se han mapeado algunas partes del cerebro y parece que a menudo las neuronas se organizan en capas consecutivas, como se muestra en la siguiente figura, que representa las múltiples capas de una RNB del córtex humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multiple_layers](images/ch10/multiple_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculos lógicos con neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warren McCulloch y Walter Pitts propusieron un modelo muy simple de neurona biológica, que posteriormente se conoció como *neurona artificial*: tenía una o más entradas binarias (on/off) y una salida binaria. La neurona artifical simplemente activa su salida cuando cierto número de sus entradas están activas. MacCulloch y Pitts mostraron que incluso con este modelo simplificado es posible construir una red de neuronas artificiales que pueden calcular cualquier proposición lógica que se quiera. Por ejemplo, construyamos algunas RNAs para ejecutar varios cálculos lógicos (ver siguiente figura), asumiendo que una neurona se activa cuando al menos dos de sus entradas están activas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rna_performing](images/ch10/rna_performing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ La primera red de la izquierda simplemente es la función identidad: si una neurona A está activada, la neurona C también se activa (dado que recibe dos señales de entrada de la neurona A), pero si la neurona A está apagada, la neurona C también está apagada.\n",
    "\n",
    "+ La segunda red ejecuta un AND lógico: la neurona C está activada solo cuando las neuronas A y B están activadas (una única señal de entrada no es suficiente para activar la neurona C).\n",
    "\n",
    "+ La tercera red ejecuta un OR lógico: la neurona C se activa si cualquiera de las neuronas A o B está activa (o ambas).\n",
    "\n",
    "+ Finalmente, si suponemos que una conexión de entrada puede inhibir la actividad de una neurona (como es el caso de las neuronas biológicas), entonces la cuarta red calcula una proposición lógica ligeramente más compleja: la neurona C es activada solo si la neurona A está activada y si la neurona B está apagada. Si la neurona A está activada todo el tiempo, entonces tendremos un NOT lógico: la neurona C está activada cuando la neurona B está apagada, y viceversa.\n",
    "\n",
    "Podemos imaginar fácilmente cómo podemos combinar estas redes para calcular expresiones lógicas complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptrón es una de las arquitecturas RNA más simples, inventada en 1957 por Frank Rosenblatt. Se basa en una neurona artificial ligeramente diferente (ver la siguiente figura) llamada *unidad lógica de umbral* (*threshold logic unit - TLU*) o algunas veces *unidad de umbral lineal* (*linear threshold unit - LTU*): las entradas y las salidas son ahora números (en lugar de valores binarios on/off) y cada conexión de entrada está asociada con un peso. La TLU calcula una suma ponderada de sus entradas ($z = w_1x_1 + w_2x_2 + \\dots + w_nx_n = x^Tw$) y luego aplica una *función de paso* a esta suma y devuelve el resultado: $h_w(x) = \\text{step}(z)$, donde $z = x^Tw$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TLU](images/ch10/TLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de paso más común usada en los perceptrones es la *función Heaviside*. Algunas veces se usa la función señal en su lugar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![heaviside](images/ch10/heaviside.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una única TLU se puede usar para clasificación binaria lineal simple. Calcula una combinación lineal de las entradas y si el resultado excede un umbral, devuelve la clase positiva y si no devuelve la clase negativa (como un clasificador de regresión logística o un SVM lineal). Por ejemplo, podemos usar una única TLU para clasificar flores de iris basándonos en la longitud y ancho del pétalo (también añadiendo una característica de sesgo extra $x_0 = 1$, como hicimos en anteriores capítulos). En este caso, entrenar una TLU significa encontrar el valor correcto para $w_0$, $w_1$ y $w_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un perceptrón se compone simplemente de una única capa de TLUs (*el nombre de perceptrón se usa algunas veces para referirse a una red pequeña con una única TLU*), con cada TLU conectada a todas las entradas. Cuando todas las neuronas de una capa están conectadas a cada neurona de la capa previa (es decir, sus neuronas de entrada), es llamada una *capa conectada completamente* o una *capa densa*. Para representar el hecho de que cada entrada se envía a cada TLU, es común dibujar neuronas de paso especiales llamadas *neuronas de entrada*: simplemente emiten cada entrada que se les proporciona. Todas las neuronas de entrada forman la *capa de entrada*. Además, generalmente se añade una característica de sesgo extra ($x_0 = 1$): es normalmente representada usando un tipo especial de neurona llamada *neurona de sesgo*, que solo emite 1 todo el tiempo. Un perceptrón con dos entradas y tres salidas está representado en la siguiente figura. Este perceptrón puede clasificar simultáneamente instancias en tres clases binarias diferentes, lo que lo convierte en una clasificador de múltiples salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron](images/ch10/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a la magia del álgebra lineal es posible calcular eficientemente las salidas de una capa de neuronas artificiales para varias instancias a la vez usando la siguiente ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_{W, b}(X) = \\phi(XW + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Como siempre, **X** representa la matriz de características de entrada. Tiene una fila por instancia y una columna por característica.\n",
    "\n",
    "+ La matriz de pesos **W** contiene todas los pesos de conexiones excepto los de la neurona de sesgo. Tiene una fila por neurona de entrada y una columna por neurona artificial en la capa.\n",
    "\n",
    "+ El vector de sesgo **b** contiene todas los pesos de conexiones entre la neurona de sesgo y las neuronas artificiales. Tiene un término de sesgo por neurona artificial.\n",
    "\n",
    "+ La función $\\phi$ se denomina *función de activación*: cuando las neuronas artificiales son TLUs, es una función de paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se entrena un perceptrón? El algoritmo de entrenamiento del perceptrón propuesto por Frank Rosenblatt se inspiró en gran medida en la *regla de Hebb*. En su libro *The organization of behavior*, publicado en 1949, Donald Hebb sugiere que cuando una neurona biológica a menudo activa otra neurona, la conexión entra ambas se fortalece. Esta idea fue posteriormente resumida por Siegrid Löwel en esta frase pegadiza: \"Las células que se disparan juntas, se conectan juntas\". Esta regla se conoció más tarde como la regla de Hebb (o *aprendizaje hebbiano*); es decir, el peso de conexión entre dos neuronas aumenta siempre que tengan la misma salida. Los perceptrones son entrenados usando una variante de esta regla que tiene en cuenta el error cometido por la red; refuerza las conexiones que ayudan a reducir este error. Más específicamente, el perceptrón recibe una instancia de entrenamiento a la vez y por cada instancia hace sus predicciones. Por cada salida de neurona que produce una predicción errónea, refuerza los pesos de conexión de las entradas que habrían contribuido a la predicción correcta. La regla se muestra en la siguiente ecuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{i,j}^{(\\text{siguiente paso})} = w_{i, j} + \\eta(y_j - \\hat{y}_j)x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $w_{i,j}$ es el peso de conexión entre la i-ésima neurona de entrada y la j-ésima neurona de salida.\n",
    "\n",
    "+ $x_i$ es el i-ésimo valor de entrada de la actual instancia de entrenamiento.\n",
    "\n",
    "+ $\\hat{y}_j$ es la salida de la j-ésima neurona de salida de la actual instancia de entrenamiento.\n",
    "\n",
    "+ $y_j$ es la salido objetivo de la j-ésima neurona de salida de la actual instancia de entrenamiento.\n",
    "\n",
    "+ $\\eta$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El límite de decisión de cada neurona de salida es lineal, por tanto los perceptrones son incapaces de patrones complejos de aprendizaje (al igual que los clasificadores de regresión logística). Sin embargo, si las instancias de entrenamiento son linealmente separables, Rosenblatt demostró que este algoritmo convergería a una solución (*tengamos en cuenta que esta solución es generalmente no única: en general cuando los datos son linealmente separables, existe una infinidad de hiperplanos que pueden separarlos*). Esto se denomina *Teorema de convergencia del perceptrón*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn suministra una clase `Perceptron` que implementa una única red de TLU. Puede ser usada muy fácilmente como cabría esperar -por ejemplo, en el dataset de iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)]  # longitud y ancho de pétalo\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n",
    "b = -per_clf.intercept_ / per_clf.coef_[0][1]\n",
    "\n",
    "axes = [0, 5, 0, 2]\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_predict = per_clf.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que hayas notado el hecho de que el algoritmo de aprendizaje del perceptrón se parece mucho al Descenso de Gradiente Estocástico. De hecho, la clase `Perceptron` de Scikit-Learn es equivalente a usar un `SGDClassifier` con los siguientes hiperparámetros: `loss=\"perceptron\"`, `learning_rate=\"constant\"`, `eta0=1` (tasa de aprendizaje) y `penalty=None` (sin regularización).\n",
    "\n",
    "Tengamos en cuenta que, al contrario que los clasificadores de regresión logística, los perceptrones no devuelven una probabilidad de clase; en su lugar, hacen predicciones basadas en un umbral duro. Esta es una de las buenas razones para preferir la regresión logística sobre los perceptrones.\n",
    "\n",
    "En su monografía de 1969 titulada *Perceptrones*, Marvin Minsky y Seymour Papert destacaron una serie de graves debilidades de los perceptrones, en particular el hecho de que eran incapaces de solventar problemas triviales (por ejemplo, el problema de clasificación de *OR exclusivo (XOR)*; ver el lado izquierdo de la siguiente figura). Por supuesto, esto también es cierto para cualquier otro modelo de clasificación lineal (como los clasificadores de regresión logística), pero los investigadores esperaban mucho más de los perceptrones y su decepción fue grande, y muchos abandoraron las redes neuronales por completo en favor de problemas de alto nivel como lógica, resolución de problemas y búsqueda.\n",
    "\n",
    "Sin embargo, resulta que algunas de las limitaciones de los perceptrones pueden eliminarse apilando múltimes perceptrones. La RNA resultante se denomina *Perceptrón Multi-Capa* (*Multi-Layer Perceptron, MLP*). En particular, un MLP puede resolver el problema XOR, como podemos comprobar calculando la salida del MLP representado en la parte derecha de la siguiente figura: con entradas (0, 0) o (1, 1) la red devuelve 0 y con entradas (0, 1) o (1, 0) devuelve 1. Todas las conexiones tienen un peso igual a 1, excepto las cuatro conexiones donde se muestra el peso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xor_mlp](images/ch10/xor_mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrón Multi-Capa y propagación hacia atrás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un MLP está compuesto de una *capa de entrada* (de paso), una o más capas de TLUs, llamadas *capas ocultas* y una capa final de TLUs denominada *capa de salida*. Las capas más cercanas a la capa de entrada se llaman normalmente capas inferiores y las más cercanas a la de salida se llaman normalmente capas superiores. Cada capa, excepto la capa de salida, incluyen una neurona de sesgo y están completamente conectadas a la siguiente capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mlp](images/ch10/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "La señal fluye solo en una única dirección (de las entradas a las salidas), por tanto esta arquitectura es un ejemplo de *red neuronal prealimentada* (*feedforward neural network - FNN*)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando una RNA contiene una pila profunda de capas ocultas (*En la década de 1990, una RNA con más de dos capas ocultas se consideraba profunda. Hoy en día es común ver RNA con docenas de capas, o incluso cientos, por lo que la definición de \"profundo\" es bastante confusa.*), se la denomina *red neural profunda* (*deep neural network - DNN*). El campo del aprendizaje profundo estudia las DNNs y más generalmente modelos conteniendo pilas profundas de cálculos. Sin embargo, muchas personas hablan de Deep Learning cuando se trata de redes neuronales (incluso las superficiales).\n",
    "\n",
    "Durante muchos años los investigadores lucharon sin éxito por encontrar una forma de entrenar MLPs. Pero en 1986, David Rumelhart, Geoffrey Hinton y Ronald Williams publicaron un [innovador artículo](https://homl.info/44) presentando el algoritmo de entrenamiento de propagación hacia atrás (*backpropagation*), que aún es usado hoy en día. En resumen, es simplemente Descenso de Gradiente utilizando una técnica eficiente para calcular automáticamente los gradientes: en solo dos pasos a través de la red (uno hacia adelante y otro hacia atrás), el algoritmo de propagación hacia atrás es capaz de calcular el gradiente del error de la red con respecto a cada parámetro del modelo. En otras palabras, puede encontrar cómo se debe ajustar cada peso de conexión y cada término de sesgo para reducir el error. Una vez que tiene estos gradientes, ejecuta el paso regular de descenso de gradiente y se repite el proceso completo hasta que la red converge a la solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "El cálculo automático de gradientes se denomina *diferenciación automática* (o *autodiff*). Existen varias técnicas de autodiff, con sus pros y sus contras. Una de ellas usada por la propagación hacia atrás se llama *reverse-mode autodiff*. Es rápida y precisa y se adapta bien cuando la función a diferenciar tiene muchas variables (por ejemplo, pesos de conexión) y pocas salidas (por ejemplo, una pérdida).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repasemos este algoritmo con un poco más de detalle:\n",
    "\n",
    "+ Maneja un mini-lote a la vez (por ejemplo contiene 32 instancias cada uno) y pasa por el conjunto de entrenamiento varias veces. Cada paso se llama *ciclo* (*epoch*), como vimos en el capítulo 4.\n",
    "\n",
    "+ Cada mini-lote se pasa a la capa de entrada de la red, que simplemente lo envía a la primera capa oculta. Después el algoritmo calcula la salida de todas las neuronas de esta capa (por cada instancia del mini-lote). El resultado se pasa a la siguiente capa, se calcula su salida y se pasa a la siguiente capa y así sucesivamente hasta que obtenemos la salida de la última capa. Es el *paso hacia adelante*: es exactamente como hacer predicciones, excepto que se conservan todos los resultados intermedios, dado que son necesarios para el paso hacia atrás.\n",
    "\n",
    "+ Después, el algoritmo mide el error de salida de la red (es decir, usa una función de pérdida que compara la salida deseada y la salida actual de la red y devuelve alguna medida del error).\n",
    "\n",
    "+ Entonces calcula cuánto contribuye cada conexión de salida al error. Esto se realiza analíticamente simplemente aplicando la *regla de la cadena* (quizás la regla más fundamental en cálculo), que hace este paso rápido y preciso.\n",
    "\n",
    "+ El algoritmo entonces mide cuántas de esas contribuciones de error provienen de cada conexión de la capa inferior, usando de nuevo la regla de la cadena -y así sucesivamente hasta que el algoritmo alcanza la capa de entrada. Como se explicó anteriormente, este paso inverso mide eficientemente el gradiente de error en todos los pesos de conexión de la red, propagando el gradiente de error hacia atrás a través de la red (de ahí el nombre del algoritmo).\n",
    "\n",
    "+ Finalmente, el algoritmo ejecuta un paso de descenso de gradiente para ajustar todos los pesos de conexión en la red, usando los gradientes de error recién calculados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo es tan importante que vale la pena resumirlo nuevamente: por cada instancia de entrenamiento el algoritmo de propagación hacia atrás primero hace una predicción (paso hacia adelante), mide el error, después pasa por cada capa en sentido inverso para medir la contribución de error de cada conexión (paso inverso) y finalmente ajusta ligeramente los pesos de conexión para reducir el error (paso de descenso de gradiente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Es importante inicializar aleatoriamente todos los pesos de conexión de las capas ocultas, de lo contrario el entrenamiento fallará. Por ejemplo, si inicializamos todos los pesos y sesgos a cero, entonces todas las neuronas en una capa serán perfectamente idénticas y, por lo tanto, la propagación hacia atrás les afectará exactamente de la misma forma, por lo que permanecerán idénticas. En otras palabras, a pesar de tener cientos de neuronas por capa, nuestro modelo actuará como si solo hubiera una neurona por capa: no será demasiado inteligente. Si, por el contrario, inicializamos aleatoriamente los pesos, *romperemos la simetría* y permitiremos que la propagación hacia atrás entrene un equipo diverso de neuronas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que este algoritmo funcione correctamente, los autores realizaron un cambio clave a la arquitectura de MLPs: reemplazaron la función de paso con la función logística, $\\sigma = 1/(1 + exp(-z))$. Esto fue esencial porque la función de paso contiene solo segmentos planos, por lo que no hay gradiente con el que trabajar (el descenso de gradiente no puede moverse en una superficie plana), mientras que la función logística tiene una derivada distina de cero bien definida en todas partes, permitiendo al descenso de gradiente progresar en cada paso. De hecho, el algoritmo de propagación hacia atrás funciona bien con muchas otras *funciones de activación*, no solo la función logística. Otras dos populares funciones de activación son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*La función tangente hiperbólica* $tanh(z) = 2\\sigma(2z) - 1$\n",
    "\n",
    "Al igual que la función logística tiene forma de S, continua y diferenciable, pero su valor de salida varía de -1 a 1 (en lugar de 0 a 1 en el caso de la función logística), lo que tiende a hacer la salida de cada capa más o menos centrada alrededor de 0 al principio del entrenamiento. Esto a menudo ayuda a acelerar la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*La función Unidad Lineal Rectificada: ReLU(z) = max(0, z)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es continua pero desafortunadamente no diferenciable en z = 0 (la pendiente cambia abruptamente, lo que puede hacer que el descenso de gradiente rebote alrededor), y su derivada es 0 para z < 0. Sin embargo, en la práctica trabaja muy bien y tiene la ventaja de ser rápida de calcular (*las neuronas biológicas parecen implementar una función de activación aproximadamente sigmoidea, en forma de S, por lo que los investigadores sintieron apego a las funciones sigmoideas durante mucho tiempo. Pero resulta que ReLU generalmente trabaja mejor en RNAs. Este es uno de esos casos donde la analogía fue engañosa*). Más importante aún, el hecho de que no tenga un valor de salida máximo también ayuda a reducir algunos problemas durante el descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas populares funciones de activación y sus derivadas se representan en la siguiente figura. ¡Pero espera! ¿Por qué necesitamos funciones de activación en primer lugar? Bueno, si encadenamos varias transformaciones lineales, todo lo que obtenemos es una transformación lineal. Por ejemplo, digamos $f(x) = 2x + 3$ y $g(x) = 5x - 1$, luego encadenar estas dos funciones lineales nos da otra función lineal: $f(g(x)) = 2(5x - 1) + 3 = 10x + 1$. Por tanto, si no tenemos algo de no linealidad entre capas, incluso una pila profunda de capas es equivalente a una única capa: no podemos resolver problemas muy complejos con esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaviside(z):\n",
    "    return (z >= 0).astype(z.dtype)\n",
    "\n",
    "def mlp_xor(x1, x2, activation=heaviside):\n",
    "    return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1s = np.linspace(-0.2, 1.2, 100)\n",
    "x2s = np.linspace(-0.2, 1.2, 100)\n",
    "x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "z1 = mlp_xor(x1, x2, activation=heaviside)\n",
    "z2 = mlp_xor(x1, x2, activation=sigmoid)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contourf(x1, x2, z1)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: heaviside\", fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contourf(x1, x2, z2)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: sigmoid\", fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡OK! Ahora sabemos de dónde vienen las redes neuronales, cuál es su arquitectura y cómo calcular sus salidas, y también hemos aprendido el algoritmo de propagación hacia atrás. Pero, ¿qué podemos hacer exactamente con ellas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPs de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, los MLPs se pueden usar en tareas de regresión. Si queremos predecir un único valor (por ejemplo, el precio de una vivienda dadas muchas de sus características), entonces solo necesitamos una única neurona de salida: su salida es el valor predicho. Para regresión multivariable (es decir, predecir múltiples valores a la vez), necesitamos una neurona de salida por cada dimensión de salida. Por ejemplo, para localizar el centro de un objeto en una imagen necesitamos predecir coordenadas 2D, por tanto, necesitamos dos neuronas de salida. Si también queremos situar una caja delimitadora alrededor del objeto, necesitaremos dos números más: el ancho y el alto del objeto. Por tanto, terminaremos con 4 neuronas de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, cuando construimos un MLP para regresión, no queremos usar ninguna función de activación para las neuronas de salida, por lo que son libres de devolver cualquier rango de valores. Sin embargo, si queremos garantizar que la salida siempre sea positiva, entonces podemos usar la función de activación ReLU o la función de activación *softplus* en la capa de salida. Finalmente, si queremos garantizar que las predicciones caerán dentro de una rango de valores dado, podemos usar la función logística o la tangente hiperbólica, y escalar las etiquetas al rango apropiado: de 0 a 1 para la función logística, de -1 a 1 para la tangente hiperbólica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalmente, la función de pérdida a usar durante el entrenamiento es el error cuadrático medio, pero si tenemos muchos valores atípicos en el conjunto de entrenamiento, podríamos preferir usar el error absoluto medio. Alternativamente, podemos usar la pérdida de Huber, que es una combinación de ambos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "La pérdida de Huber es cuadrática cuando el error es más pequeño que un umbral $\\delta$ (normalmente 1), pero lineal cuando el error es mayor que $\\delta$. Esto lo hace menos sensible a los valores atípicos que el error cuadrático medio y a menudo es más preciso y converge más rápidamente que el error absoluto medio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se muestra un resumen de la arquitectura típica de un MLP de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regression_mlp](images/ch10/regresion_mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPs de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los MLPs también pueden usarse en tareas de clasificación. Para un problema de clasificación binaria, solo necesitamos una única neurona de salida usando la función de activación logística: la salida será un número de 0 a 1, que podemos interpretar como la probabilidad estimada de la clase positiva. Obviamente, la probabilidad estimada de la clase negativa es igual a uno menos ese número.\n",
    "\n",
    "Los MLPs también pueden manejar fácilmente tareas de clasificación binaria multietiqueta. Por ejemplo, podríamos tener un sistema de clasificación de email que predice si cada correo entrante es o no spam y, simultáneamente, predice si se trata de un email urgente o no urgente. En este caso, necesitaríamos dos neuronas de salida, ambas usando la función de activación logística: la primera devolvería la probabilidad de que el email sea spam y la segunda devolvería la probabilidad de que sea urgente. Más genéricamente, dedicaríamos una neurona de salida por cada clase positiva. Tengamos en cuenta que las probabilidades de salida no necesariamente se suman. Esto permite que el modelo devuelva cualquier combinación de etiquetas: podemos tener correo bueno no urgente, bueno urgente, spam no urgente e incluso spam urgente (aunque esto último probablemente sería un error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cada instancia puede pertenecer solo a una única clase, de 3 ó más clases posibles (por ejemplo, clases 0 a 9 para la clasificación de imágenes de dígitos), necesitamos tener una neurona de salida por clase y deberíamos utilizar la función de activación *softmax* para la capa de salida completa. La función softmax (presentada en el capítulo 4) garantiza que todas las probabilidades estimadas se encuentran entre 0 y 1 y que sumen 1 (lo cual es necesario si las clases son exclusivas). Esto se denomina clasificación multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mlp_classification](images/ch10/mlp_classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con respecto a la función de pérdida, dado que estamos prediciendo distribuciones de probabilidad, la entropía cruzada (también llamada perdida logarítmica, ver capítulo 4) es normalmente una buena elección.\n",
    "\n",
    "La siguiente tabla resume la arquitectura típica de un MLP de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mlp_classification_arq](images/ch10/mlp_classification_arqu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Antes de seguir adelante, recomendamos realizar el ejercicio 1, al final del capítulo. Jugaremos con varias arquitecturas de redes neuronales y visualizaremos sus salidas usando *TensorFlow Playground*. Resultará muy útil para comprender mejor los MLPs, por ejemplo, los efectos de todos los hiperparámetros (número de capas y neuronas, funciones de activación y demás).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos todos los conceptos que necesitamos para empezar implementando MLPs con Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando MLPs con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras es una API de Deep Learning de alto nivel que permite construir, entrenar, evaluar y ejecutar fácilmente todo tipo de redes neuronales. Su documentación (o especificación) está disponible en https://keras.io. La implementación de referencia también se denomina Keras, por lo que para evitar confusiones lo llamaremos keras-team (ya que está disponible en https://github.com/keras-team/keras). Fue desarrollado por François Chollet como parte de un proyecto de investigación (Proyecto ONEIROS - Open-ended Neuro-Electronic Intelligent Robot Operating System) y liberado como proyecto de open source en marzo de 2015. Rápidamente ganó popularidad debido a su facilidad de uso, flexibilidad y bonito diseño. Para ejecutar los cálculos pesados requeridos por las redes neuronales, el equipo de keras se basó en backend de cálculo. En la actualidad, podemos elegir entre tres librerías de deep learning de código abierto: TensorFlow, Microsoft Cognitive Toolkit (CNTK) y Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, desde finales de 2016 se han liberado otras implementaciones. Ahora podemos ejecutar Keras en Apache MXNet, Core ML de Apple, Javascript o Typescript (ejecutar código Keras en un navegador web) o PlaidML (que se puede ejecutar en todo tipo de GPUs, no solo Nvidia). Además, TensorFlow ahora viene incluido con su propia implementación de Keras, llamada tf.keras. Solo admite TensorFlow como backend, pero tiene la ventaja de ofrecer algunas características extras muy útiles (ver la siguiente figura): por ejemplo, es compatible con la API de datos de TensorFlow, lo que permite cargar y preprocesar datos de una manera más fácil y eficiente. Por esta razón, usaremos tf.keras en este libro. Sin embargo, en este capítulo no usaremos ninguna de las características específicas de TensorFlow, por lo que el código debería ejecutarse correctamente en cualquier otra implementación de Keras (al menos en Python), con solo modificaciones menores, como cambiar los *import*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kear_impl](images/ch10/keras_impl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tf.keras está incluido con TensorFlow, instalemos TensorFlow.\n",
    "\n",
    "---\n",
    "\n",
    "Para soporte GPU, necesitaremos instalar `tensorflow-gpu` en lugar de `tensorflow`, y existen otras librerías a instalar. Para más detalle, ver https://tensorflow.org/install/gpu.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a comprobar la versión instalada\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir un clasificador de imágenes usando la API secuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, necesitamos cargar un dataset. Abordaremos el *MNIST Fashion*, que es un reemplazo directo del MNIST. Tiene exactamente el mismo formato que MNIST (70.000 imágenes en escala de grises de 28x28 píxeles cada una, con 10 clases), pero las imágenes representan elementos de moda, en lugar de dígitos escritos a mano, por lo que cada clase es más diversa y el problema resulta significativamente más desafiante que MNIST. Por ejemplo, un modelo lineal simple alcanza aproximadamente un 92% de precisión en MNIST y solo alrededor del 83% en el MNIST Fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando Keras para cargar el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras proporciona algunas funciones útiles para cargar datasets comunes, incluyendo MNIST, MNIST Fashion, el dataset original de viviendas de California y más. Carguemos MNIST Fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando cargamos MNIST o MNIST Fashion usando Keras en lugar de Scikit-Learn, una diferencia importante es que cada imagen está representada como una matriz de 28x28, en lugar de una matriz 1D de tamaño 784. Además, las intensidades de los píxeles están representadas como enteros (de 0 a 255), en lugar de floats (de 0.0 a 255.0). A continuación se muestra el tamaño y el tipo de los datos del conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengamos en cuenta que el dataset ya está dividido entre conjunto de entrenamiento y conjunto de prueba, pero no existe conjunto de validación, así que lo vamos a crear. Además, dado que vamos a entrenar la red neuronal usando descenso de gradiente, debemos escalar las características de entrada. Por simplicidad, vamos a escalar solo las intensidades de los píxeles al rango 0-1 dividiéndolas por 255.0 (esto también las convierte a floats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos dibujar una imagen usando la función `imshow()` de Matplotlib, con un mapa de color `'binario'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas son los IDs de las clases (representados como enteros - uint8), del 0 al 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con MNIST, cuando la etiqueta es igual a 5, significa que la imagen representa el dígito 5 escrito a mano. Fácil. Sin embargo, para MNIST Fashion necesitamos listar los nombres de las clases para saber a qué nos estamos refiriendo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, la primera imagen del conjunto de entrenamiento representa un abrigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de validación contiene 5.000 imágenes y el conjunto de test 10.000 imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a un ejemplo de las imágenes del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x345.6 with 40 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando el modelo usando la API secuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora construyamos la red neuronal. Aquí tenemos un MLP de clasificación con dos capas ocultas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ir línea por línea:\n",
    "\n",
    "+ La primera línea crea un modelo `Sequential`. Este es el modelo de Keras más simple para redes neuronales que solo están compuestas de una única pila de capas, conectadas secuencialmente. Esto se denomina API secuencial.\n",
    "\n",
    "+ Despues, construimos la primera capa y la añadimos al modelo. Es una capa `Flatten` cuyo papel es simplemente covertir cada imagen de entrada en una matriz 1D: si recibe los datos de entrada X, calcula `X.reshape(-1, 1)`. Esta capa no tiene ningún parámetro, está ahí solo para hacer un simple preprocesamiento. Dado que es la primera capa del modelo, se debe especificar el `input_shape`: no incluye el tamaño del lote, solo la forma de las instancias. Alternativamente, podríamos agregar un `keras.layers.InputLayer` como primera capa, estableciéndo `shape=[28,28]`.\n",
    "\n",
    "+ Después añadimos una capa `Dense` con 300 neuronas. Usaremos la función de activación ReLU. Cada capa `Dense` gestiona su propia matriz de pesos, conteniendo todos los pesos de conexión entre las neuronas y sus entradas. También gestiona un vector de términos de sesgos (uno por neurona). Cuando recibe algún dato de entrada, calcula la ecuación que vimos anteriormente: $h_{W, b}(X) = \\phi(XW + b)$\n",
    "\n",
    "+ Lo siguiente añadimos una segunda capa `Dense` oculta con 100 neuronas, también usando la función de activación ReLU.\n",
    "\n",
    "+ Finalmente, añadimos una capa `Dense` de salida con 10 neuronas (una por clase), usando la función de activación softmax (porque las clases son exlusivas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Especificar `activation=\"relu\"` es equivalente a `activation=keras.activations.relu`. Existen otras funciones de activación disponibles en el paquete `keras.activations.relu`, usaremos muchas de ellas en este libro. Para un listado completo ver https://keras.io/activations/.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de añadir las capas una por una como hemos hecho, podemos pasar un listado de capas cuando creemos el modelo `Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Uso de ejemplos de código de keras.io**\n",
    "\n",
    "Los ejemplos de código documentados en keras.io funcionarán correctamente con tf.keras, pero necesitarás cambiar los import. Por ejemplo, si consideramos el siguiente código de keras.io:\n",
    "\n",
    "    from keras.layers import Dense\n",
    "    output_layer = Dense(10)\n",
    "    \n",
    "Debemos cambias los import de la siguiente forma:\n",
    "\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    output_layer = Dense(10)\n",
    "    \n",
    "O simplemente usar full path:\n",
    "\n",
    "    from tensorflow import keras\n",
    "    output_layer = keras.layers.Dense(10)\n",
    "    \n",
    "Esto es más detallado, pero usaremos este enfoque en este libro para que veamos fácilmente cuáles son lo paquetes que estamos usando y evitar la confusión entre las clases estándar y las clases personalizadas. En código de producción es lo que utiliza la mayoría de la gente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `summary()` del modelo muestra todas las capas del modelo (*también podemos generar una imagen de nuestro modelo usando `keras.util.plot.model()`*), incluyendo el nombre de cada capa (que se genera automáticamente, a menos que se establezca cuando se crea la capa), su forma de salida (`None` significa que el tamaño del lote puede ser cualquiera) y su número de parámetros. El resumen termina con el número total de parámetros, incluyendo los parámetros entrenables y no entrenables. Aquí solo tenemos parámetros entrenables (más adelante en otros capítulos veremos ejemplos de parámetros no entrenables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos que las capas `Dense` a menudo tienen muchos parámetros. Por ejemplo, la primera capa oculta tiene 784 x 300 pesos de conexión, más 300 términos de sesgo, lo que suma hasta 235.000 parámetros. Esto le da al modelo bastante flexibilidad para adaptarse a los datos de entrenamiento, pero también significa que el modelo corre el riesgo de sufrir sobreajuste, especialmente cuando no tiene muchos datos de entrenamiento. Volveremos a esto más tarde.\n",
    "\n",
    "Podemos obtener fácilmente la lista de capas del modelo, accediendo por su índice o por su nombre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x1c2a8175340>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1c2a81759a0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1c2a8175ca0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1c2a81751c0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_2'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(\"dense_2\").name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los parámetros de una capa son accesibles usando los métodos `get_weights()` y `set_weights()`. Para una capa `Dense` esto incluye tanto los pesos de conexión como los términos de sesgo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02448617, -0.00877795, -0.02189048, ..., -0.02766046,\n",
       "         0.03859074, -0.06889391],\n",
       "       [ 0.00476504, -0.03105379, -0.0586676 , ...,  0.00602964,\n",
       "        -0.02763776, -0.04165364],\n",
       "       [-0.06189284, -0.06901957,  0.07102345, ..., -0.04238207,\n",
       "         0.07121518, -0.07331658],\n",
       "       ...,\n",
       "       [-0.03048757,  0.02155137, -0.05400612, ..., -0.00113463,\n",
       "         0.00228987,  0.05581069],\n",
       "       [ 0.07061854, -0.06960931,  0.07038955, ..., -0.00384101,\n",
       "         0.00034875,  0.02878492],\n",
       "       [-0.06022581,  0.01577859, -0.02585464, ..., -0.00527829,\n",
       "         0.00272203, -0.06793761]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos que la capa `Dense` está inicializada con pesos de conexión aleatorios (necesario para romper la simetría, como vimos anterioresmente) y que los sesgos son inicializados a ceros, lo que es correcto. Si quisiéramos utilizar un método de inicialización diferente, podemos establecer `kernel_initializer` (*kernel* es otro nombre para la matriz de pesos de conexión) o `bias_initializer` cuando creamos la capa. Hablaremos sobre los inicializadores en otros capítulos, pero si queremos ver un listado completo de inicializadores por ir a https://keras.io/initializers/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "La forma de la matriz de pesos depende del número de entradas. Esto es por lo que es recomendable especificar `input_shape` cuando creamos la primera capa en un modelo `Sequential`. Sin embargo, si no lo especificamos, está bien: Keras simplemente esperará hasta conocer el tamaño de entrada antes de construir realmente el modelo. Esto sucederá tanto cuando le proporcionemos datos reales (por ejemplo, durante el entrenamiento) o cuando llamemos al método `build()`. Hasta que el modelo sea realmente construido, las capas no tendrán ningún peso y no serán capaces de hacer ciertas cosas (como mostrar el resumen del modelo o salvar el modelo), así que si sabemos el tamaño de la entrada cuando creamos el modelo es mejor especificarlo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de crear el modelo, debemos llamar al método `compile()` para especificar la función de pérdida y el optimizador a usar. Opcionalmente, también podemos especificar una lista de métricas extras a calcular durante el entrenamiento y la evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Usar `loss=\"sparse_categorical_crossentropy\"` es equivalente a `loss=keras.losses.sparse_categorical_crossentropy`. De forma similar, `optimizer=\"sgd\"` es equivalente a `optimizer=keras.optimizers.SGD()` y `metrics=[\"accuracy\"]` es equivalente a `metrics=[keras.metrics.sparse_categorical_accuracy]` (cuando usamos esta pérdida). Usaremos otras muchas pérdidas, optimizadores y métricas, pero para una lista completa ver: https://keras.io/losses/, https://keras.io/optimizers/ y https://keras.io/metrics/.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto requiere alguna explicación. En primer lugar, usamos la pérdida `\"sparse_categorical_crossentropy\"` porque tenemos etiquetas dispersas (es decir, por cada instancia hay solo un índice de clase objetivo, de 0 a 9 en este caso) y las clases son exclusivas. Si en cambio tuviéramos una probabilidad objetivo por clase por cada instancia (tal como vectores one-hot, es decir, `[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]` para representar la clase 3), entonces necesitaríamos usar en su lugar la pérdida `\"categorical_crossentropy\"`. Si hiciéramos clasificación binaria (con una o más etiquetas binarias) entonces tendríamos que usar la función de activación `\"sigmoid\"` (es decir, logística) en la capa de salida, en lugar de la función de activiación `\"softmax\"` y usaríamos la pérdida `\"binary_crossentropy\"`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Si queremos convertir etiquetas dispersas (es decir, índices de clases) a vectores one-hot, podemos usar la función `keras.utils.to_categorical()`. Para convertir al revés, podemos usar la función `np.argmax()` con `axis=1`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segundo lugar, con respecto al optimizador, `\"sgd\"` significa simplemente que entraremos el modelo usando Descenso de Gradiente Estocástico. En otras palabras, Keras ejecutará el algoritmo de propagación hacia atrás descrito anteriormente. \n",
    "\n",
    "Finalmente, dado que esto es un clasificador, es útil medir su `\"accuracy\"` durante el entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el modelo está listo para ser entrenado. Para ello, simplemente necesitamos llamar a su método `fit()`. Pasamos sus características de entrada (`X_train`) y las clases objetivo (`y_train`), así como el número de ciclos de entrenamiento (o si no, por defecto será solo 1, lo que definitivamente no será suficiente para converger a un buena solución). También pasamos un conjunto de validación (esto es opcional): Keras medirá la pérdida y las métricas extras en este conjunto al término de cada ciclo, lo que resulta muy útil para ver realmente cómo de bien se está ejecutando el modelo: si la ejecución en el conjunto de entrenamiento es mucho mejor que en el conjunto de validación, probablemente nuestro modelo estará sobreajustado al conjunto de entrenamiento (o existe un bug, como datos no coincidentes entre el conjunto de entrenamiento y el de validación):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7237 - accuracy: 0.7643 - val_loss: 0.5213 - val_accuracy: 0.8226\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4842 - accuracy: 0.8316 - val_loss: 0.4351 - val_accuracy: 0.8514\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4391 - accuracy: 0.8455 - val_loss: 0.5269 - val_accuracy: 0.8008\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4123 - accuracy: 0.8567 - val_loss: 0.3915 - val_accuracy: 0.8648\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3939 - accuracy: 0.8620 - val_loss: 0.3746 - val_accuracy: 0.8694\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3750 - accuracy: 0.8675 - val_loss: 0.3710 - val_accuracy: 0.8734\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3630 - accuracy: 0.8713 - val_loss: 0.3617 - val_accuracy: 0.8726\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3515 - accuracy: 0.8752 - val_loss: 0.3858 - val_accuracy: 0.8620\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3412 - accuracy: 0.8789 - val_loss: 0.3582 - val_accuracy: 0.8720\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3319 - accuracy: 0.8821 - val_loss: 0.3419 - val_accuracy: 0.8784\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3238 - accuracy: 0.8839 - val_loss: 0.3453 - val_accuracy: 0.8786\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3147 - accuracy: 0.8864 - val_loss: 0.3304 - val_accuracy: 0.8822\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3077 - accuracy: 0.8898 - val_loss: 0.3276 - val_accuracy: 0.8870\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3018 - accuracy: 0.8918 - val_loss: 0.3397 - val_accuracy: 0.8790\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2943 - accuracy: 0.8938 - val_loss: 0.3219 - val_accuracy: 0.8852\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2887 - accuracy: 0.8972 - val_loss: 0.3097 - val_accuracy: 0.8904\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2835 - accuracy: 0.8979 - val_loss: 0.3568 - val_accuracy: 0.8728\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2774 - accuracy: 0.9005 - val_loss: 0.3137 - val_accuracy: 0.8916\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2724 - accuracy: 0.9023 - val_loss: 0.3112 - val_accuracy: 0.8916\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2670 - accuracy: 0.9037 - val_loss: 0.3260 - val_accuracy: 0.8808\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2621 - accuracy: 0.9058 - val_loss: 0.3048 - val_accuracy: 0.8940\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2575 - accuracy: 0.9076 - val_loss: 0.2961 - val_accuracy: 0.8982\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2533 - accuracy: 0.9081 - val_loss: 0.2982 - val_accuracy: 0.8930\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2481 - accuracy: 0.9105 - val_loss: 0.3066 - val_accuracy: 0.8896\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2439 - accuracy: 0.9130 - val_loss: 0.2977 - val_accuracy: 0.8950\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2402 - accuracy: 0.9133 - val_loss: 0.3059 - val_accuracy: 0.8892\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2360 - accuracy: 0.9155 - val_loss: 0.3018 - val_accuracy: 0.8952\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2324 - accuracy: 0.9163 - val_loss: 0.2990 - val_accuracy: 0.8924\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2281 - accuracy: 0.9186 - val_loss: 0.3029 - val_accuracy: 0.8934\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2245 - accuracy: 0.9198 - val_loss: 0.3029 - val_accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Y esto es! La red neuronal está entrenada. En cada ciclo durante el entrenamiento, Keras muestra el número de instancias procesadas hasta ahora (con una barra de progreso), el tiempo entrenamiento promedio por ejemplo, la pérdida y la precisión (o cualquier otra métrica extra que hayamos indicado), tanto en el conjunto de entrenamiento como en el de validación. Podemos ver que la pérdida de entrenamiento disminuyó, lo que es buena señal y la precisión de validación alcanzó 89,32% tras 30 ciclos, no demasiado lejos de la precisión de entrenamiento, por lo que no parece haber mucho sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "En lugar de pasar un conjunto de validación usando el argumento `validation_data`, podemos establecer en su lugar `validation_split` al ratio del conjunto de entrenamiento que queremos que use Keras para validación (por ejemplo, 0.1).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 30, 'steps': 1719}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el conjunto de entrenamiento estaba muy sesgado, con algunas clases sobrerepresentadas y otras subrepresentadas, sería útil establecer el argumento `class_weight` cuando llamamos al método `fit()`, proporcionando un mayor peso a las clases subrepresentadas y un menor peso a las clases sobrerepresentadas- Estos pesos serían usados por Keras cuando calcula la pérdida. Si en cambio necesitamos pesos por instancia, podemos establecer el argumento `sample_weight` (reemplaza a `class_weight`). Esto podría ser útil por ejemplo si algunas instancias fueron etiquetas por expertos mientras que otras fueron etiquetadas usando una plataforma de crowdsourcing: es posible que queramos dar más peso a las primeras. También podemos suministrar pesos de muestra (no pesos de clase) para el conjunto de validación añadiendo un tercer ítem en la tupla `validation_data`.\n",
    "\n",
    "El método `fit()` devuelve un objeto `History` conteniendo los parámetros de entrenamiento (`history.params`), la lista de ciclos por lo que ha pasado (`history.epoch`) y. más importante, un diccionario (`history.history`) conteniendo las pérdidas y métricas extras medidas al término de cada ciclo en el conjunto de entrenamiento y en el conjunto de validación (si lo hubiera). Si creamos un DataFrame de Pandas usando este diccionario y llamamos a su método `plot()` obtendríamos las curvas de aprendizaje mostrada en la siguiente figura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tanto la precisión del entrenamiento como de la validación aumentan durante el entrenamiento, mientras que la pérdida de ambos disminuye. ¡Bien! Además, las curvas de validación están bastante cerca de las curvas de entrenamiento, lo que significa que no hay demasiado sobreajuste. En este caso particular, el modelo funcionó mejor en el conjunto de validación que en el conjunto de entrenamiento al comienzo del entrenamiento: esto a veces sucede por casualidad (especialmente cuando el conjunto de validación es bastante pequeño). Sin embargo, el rendimiento del conjunto de entrenamiento termina superando el rendimiento de validación, como suele ser el caso cuando se entrena durante el tiempo suficiente. Puede decirse que el modelo aún no ha convergido del todo, ya que la pérdida de validación aún está disminuyendo, por lo que probablemente deberíamos continuar entrenando. Es tan simple como volver a llamar al método `fit()`, ya que Keras simplemente continúa entrenando donde lo dejó (debería poder alcanzar una precisión de validación cercana al 89%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no estamos satisfechos con el rendimiento de nuestro modelo, podemos volver atra´s y ajustar los hiperparámetros del modelo, por ejemplo, el número de capas, el número de neuronas por capa, los tipos de las funciones de activación que usamos para cada capa oculta, el número de ciclos de entrenamiento, el tamaño del lote (puede establecerse en el método `fit()` usando el argumento `batch_size`, siendo por defecto 32). Volveremos atrás para ajustar los hiperparámetros al final del capítulo. Una vez que estamos satisfechos con la precisión de validación de nuestro modelo, debemos evaluarlo en el conjunto de prueba para estimar el error de generalización antes de desplegarlo en producción. Podemos hacer esto fácilmente usando el método `evaluate()` (también admite otros argumentos, como `batch_size` o `sample_weight`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 764us/step - loss: 0.3366 - accuracy: 0.8826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3366280198097229, 0.8826000094413757]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en el capítulo 2, es normal obtener una ejecución ligeramente inferior en el conjunto de prueba que en conjunto de validación, debido a que los hiperparámetros se han ajustado en el conjunto de validación, no en el de prueba (sin embargo, en este ejemplo, no hemos hecho ningún ajuste todavía, así que la menor precisión es solo mala suerte). Recordemos que debemos resistirnos a la tentación de ajustar los hiperparámetros en el conjunto de prueba o si no nuestra estimación del error de generalización será demasiado optimista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando el modelo para hacer predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, podemos usar el método `predic()` del modelo para hacer predicciones en nuevas instancias. Dado que no tenemos realmente nuevas instancias, usaremos las 3 primeras instancias del conjunto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.96],\n",
       "       [0.  , 0.  , 0.98, 0.  , 0.02, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, para cada instancia del modelo estima una probabilidad por clase, desde la clase 0 a la clase 9. Por ejemplo, para la primera imagen estima que la probabilidad de clase 9 (botines) es 96%, la probabilidad de la clase 7 (zapatillas) es 2%, la probabilidad de la clase 5 (sandalias) es 1% y para las otras clases es despreciable. En otras palabras, \"cree\" que es un zapato, probablemente botines, pero no está completametne seguro. Si solo nos importa la clase con la probabilidad estimada más alta (incluso aunque esta probabilidad fuera bastante baja) entonces podemos usar el método `predict_classes()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-44-81ace37e545f>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y realmente el clasificador ha clasificado las tres imágenes correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 518.4x172.8 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sabemos cómo construir, entrenar, evaluar y usar un MLP de clasificación usando la API Secuencial. Pero, ¿que pasa con la regresión?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo un MLP de regresión usando la API Secuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a cambiar al problema de la vivienda de California y abordarlo usando una red reuronal de regresión. Por simplicidad, usaremos la función `fetch_california_housing()` de Scikit-Learn para cargar los datos: este dataset es más simple que el que usamos en el capítulo 2, dado que solo contiene caractarísticas numéricas (no existe la característica `ocean_proximity`) y no hay valores ausentes. Después de cargar los datos, los dividiremos en un conjunto de entrenamiento, de validación y de prueba, y escalaremos todas las características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir, entrenar, evaluar y usar un MLP de regresión usando API secuencia para hacer predicciones es bastante similar a lo que hicimos para la clasificación. La principal diferencia es el hecho de que la capa de salida tiene una única neurona (dado que solo queremos predecir un único valor) y no utiliza función de activación, y la función de pérdida es el error cuadrático medio. Dado que el dataset es bastante ruidoso, usaremos una única capa oculta con menos neuronas que antes, para evitar el sobreajuste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 821us/step - loss: 1.6419 - val_loss: 0.8560\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.7047 - val_loss: 0.6531\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.6345 - val_loss: 0.6099\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.5977 - val_loss: 0.5658\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.5706 - val_loss: 0.5355\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.5472 - val_loss: 0.5173\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.5288 - val_loss: 0.5081\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.5130 - val_loss: 0.4799\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4992 - val_loss: 0.4690\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.4875 - val_loss: 0.4656\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.4777 - val_loss: 0.4482\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4688 - val_loss: 0.4479\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4615 - val_loss: 0.4296\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4547 - val_loss: 0.4233\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4488 - val_loss: 0.4176\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.4435 - val_loss: 0.4123\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.4389 - val_loss: 0.4071\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4347 - val_loss: 0.4037\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.4306 - val_loss: 0.4000\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.4273 - val_loss: 0.3969\n",
      "  1/162 [..............................] - ETA: 0s - loss: 0.4649WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "162/162 [==============================] - 0s 395us/step - loss: 0.4212\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el API secuencial es bastante fácil de usar. Sin embargo, a pesar de que los modelos secuenciales son extremadamente comunes, algunas veces es útil construir redes neuronales con topologías más complejas o con múltiples entradas o salidas. Para este propósito, Keras ofrece la API Funcional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3885664],\n",
       "       [1.6792021],\n",
       "       [3.1022797]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo modelos complejos usando la API Funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de red neuronal no secuencial es una red neuronal *Wide & Deep*. Esta arquitectura de red reuronal fue presentanda por Heng-Tze Cheng en su [artículo de 2016](https://arxiv.org/abs/1606.07792). Conecta todas o parte de las entradas directamente a la capa de salida, como se muestra en la siguiente figura. Esta arquitectura posibilita que las redes neuronables aprendan tanto patrones profundos (usando rutas profundas) como reglas simples (a través de rutas cortas). Por el contario, un MLP regular fuerza que todos los datos fluyan a través de la pila completa de capas, por lo que patrones simples en los datos pueden terminar distorsionados por esta secuencia de transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wide_deep_network](images/ch10/wide_deep_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir dicha red neuronal para abordar el problema de la vivienda de California:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vayamos línea a línea de este código:\n",
    "\n",
    "+ Primero, necesitamos crear un objeto `Input`. Esto es necesario porque podemos tener múltiples entradas, como veremos más adelante.\n",
    "\n",
    "+ Despues, creamos una capa `Dense` con 30 neuronas y usamos la función de activación ReLU. Tan pronto como es creada, observemos que la llamamos como una función, pasándole la entrada. Esto es por lo que se la denomina API Funcional. Observemos que solo le estamos diciendo a Keras cómo conectar las capas juntas, todavía no se están procesando datos reales.\n",
    "\n",
    "+ Entonces creamos una segunda capa oculta y de nuevo la usamos como una función. Observemos, sin embargo, que le pasamos la salida de la primera capa oculta.\n",
    "\n",
    "+ Después, creamos una capa `Concatenate()` y de nuevo la usamos inmediatamente como una función para concatenar la entrada y la salida de la segunda capa oculta (podríamos preferir la función `keras.layers.concatenate()` que crea una capa `Concatenate` e inmediatamente la llama con las entradas dadas).\n",
    "\n",
    "+ Después creamos la capa de salida, con una única neurona y sin función de activación, y la llamamos como una función, pasándole el resultado de la concatenación.\n",
    "\n",
    "+ Por último, creamos un `Model` Keras, especificando tanto las entradas como las salidas a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 30)           270         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           930         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos construido el modelo Keras, es exactamente igual que anteriormente, por lo que no es necesario repetirlo aquí: debemos compilar el modelo, entrenarlo, evaluarlo y hacer predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 1.3558WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "363/363 [==============================] - 0s 879us/step - loss: 1.2611 - val_loss: 3.3940\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.6580 - val_loss: 0.9360\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.5878 - val_loss: 0.5649\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.5582 - val_loss: 0.5712\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.5347 - val_loss: 0.5045\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.5158 - val_loss: 0.4831\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.5002 - val_loss: 0.4639\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 678us/step - loss: 0.4876 - val_loss: 0.4638\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.4760 - val_loss: 0.4421\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.4659 - val_loss: 0.4313\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.4577 - val_loss: 0.4345\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.4498 - val_loss: 0.4168\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.4428 - val_loss: 0.4230\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.4366 - val_loss: 0.4047\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.4307 - val_loss: 0.4078\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.4257 - val_loss: 0.3938\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.4210 - val_loss: 0.3952\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.4167 - val_loss: 0.3860\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4121 - val_loss: 0.3827\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.4088 - val_loss: 0.4054\n",
      "162/162 [==============================] - 0s 414us/step - loss: 0.4032\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero qué pasa si queremos enviar un subconjunto de las características a través de la ruta corta y un subconjunto diferente (posiblemente superpuesto) a través de la ruta profunda (véase la siguiente figura). En este caso, una solución es usar múltiples entradas. Por ejemplo, supongamos que queremos enviar 5 características a través de la ruta profunda (características 0 a 4) y 6 características a través de la ruta corta (caractarísticas 2 a 7):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multiple_inputs](images/ch10/multiple_inputs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código es autoexplicativo. Observemos que especificamos `inputs=[input_A, input_B]` cuando creamos el modelo. Ahora podemos compilar el modelo como de costumbre, pero cuando llamamos al método `fit()`, en lugar de pasar una única matriz de entrada `X_train`, debemos pasar un par de matrices `(X_train_A, X_train_B)`: una por entrada. Lo mismo  para `X_valid`, y para `X_test` y `X_new` cuando llamamos a `evalueate()` o `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.8145 - val_loss: 0.8072\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.6771 - val_loss: 0.6658\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 675us/step - loss: 0.5979 - val_loss: 0.5687\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 686us/step - loss: 0.5584 - val_loss: 0.5296\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 675us/step - loss: 0.5334 - val_loss: 0.4993\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.5120 - val_loss: 0.4811\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.4970 - val_loss: 0.4696\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4843 - val_loss: 0.4496\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4730 - val_loss: 0.4404\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.4644 - val_loss: 0.4315\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.4570 - val_loss: 0.4268\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.4510 - val_loss: 0.4166\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.4462 - val_loss: 0.4125\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4421 - val_loss: 0.4074\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4385 - val_loss: 0.4044\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.4356 - val_loss: 0.4007\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.4322 - val_loss: 0.4013\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.4305 - val_loss: 0.3987\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.4274 - val_loss: 0.3934\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4261 - val_loss: 0.4204\n",
      "162/162 [==============================] - 0s 420us/step - loss: 0.4219\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen muchos casos de uso en los cuales querríamos tener múltiples salidas:\n",
    "\n",
    "+ La tarea puede demandarlo, por ejemplo, podemos querer localizar y clasificar el objeto principal de una fotografía. Esto es tanto una tarea de regresión (encontrar las coordenadas del centro del objeto, así como su ancho y alto) y una tarea de clasificación.\n",
    "\n",
    "+ De forma similar, podríamos tener que ejecutar múltiples tareas independientes basadas en los mismos datos. Claro, podríamos entrenar una red neuronal por tarea, pero en la mayoría de los casos obtendremos mejores resultados en todas las tareas entrenando una única red neuronal con una salida por tarea. Esto es debido a que la red neuronal puede aprender características en los datos que son útiles para el resto de tareas.\n",
    "\n",
    "+ Otro caso de uso es una técnica de regularización (es decir, una restricción de entrenamiento cuyo objetivo es reducir el sobreajuste y, por tanto, mejorar la habilidad del modelo para generalizar). Por ejemplo, podríamos añadir algunas salidas auxiliares en una arquitectura de red neuronal (ver la siguiente figura) para asegurar que la parte subyacente de la red aprende algo útil por sí misma, sin depender del resto de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![auxiliar_output_regularization](images/ch10/auxiliar_output_regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir salidas extra es bastante fácil: solo hay que conectarlas a las capas apropiadas y añadirlas a la lista de salidas de nuestro modelo. Por ejemplo, el código siguiente construye la red representada en la figura anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada salida necesitará su propia función de pérdida, por lo tanto cuando compilemos el modelo debemos pasarle una lista de pérdidas (si pasamos una única pérdida, Keras asumirá que se debe usar la misma pérdida para todas las salidas). Por defecto, Keras calculará todas esas pérdidas y simplemente las sumará para obtener la pérdida final utilizada para el entrenamiento. Sin embargo, nos importa mucho más la salida principal que la salida auxiliar (ya que solo se usa para regularización), por lo que queremos darle a la pérdida de la salida principal un peso mucho mayor. Por suerte, es posible establecer todos los pesos de pérdida cuando compilamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cuando entrenemos el modelo, necesitamos proporcionarle algunas etiquetas para cada salida. En este ejemplo, la salida principal y la auxiliar deben intentar predecir lo mismo, por lo que deben usar las mismas etiquetas. En lugar de pasar `y_train`, necesitaremos pasar `(y_train, y_train)` (y lo mismo para `y_valid` e `y_test`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.1365 - main_output_loss: 1.9196 - aux_output_loss: 4.0890 - val_loss: 1.6233 - val_main_output_loss: 0.8468 - val_aux_output_loss: 8.6117\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.8905 - main_output_loss: 0.6969 - aux_output_loss: 2.6326 - val_loss: 1.5163 - val_main_output_loss: 0.6836 - val_aux_output_loss: 9.0109\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.7429 - main_output_loss: 0.6088 - aux_output_loss: 1.9499 - val_loss: 1.4639 - val_main_output_loss: 0.6229 - val_aux_output_loss: 9.0326\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.6771 - main_output_loss: 0.5691 - aux_output_loss: 1.6485 - val_loss: 1.3388 - val_main_output_loss: 0.5481 - val_aux_output_loss: 8.4552\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.6381 - main_output_loss: 0.5434 - aux_output_loss: 1.4911 - val_loss: 1.2177 - val_main_output_loss: 0.5194 - val_aux_output_loss: 7.5030\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.6079 - main_output_loss: 0.5207 - aux_output_loss: 1.3923 - val_loss: 1.0935 - val_main_output_loss: 0.5106 - val_aux_output_loss: 6.3396\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.5853 - main_output_loss: 0.5040 - aux_output_loss: 1.3175 - val_loss: 0.9918 - val_main_output_loss: 0.5115 - val_aux_output_loss: 5.3151\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.5666 - main_output_loss: 0.4898 - aux_output_loss: 1.2572 - val_loss: 0.8733 - val_main_output_loss: 0.4733 - val_aux_output_loss: 4.4740\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.5504 - main_output_loss: 0.4771 - aux_output_loss: 1.2101 - val_loss: 0.7832 - val_main_output_loss: 0.4555 - val_aux_output_loss: 3.7323\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.5373 - main_output_loss: 0.4671 - aux_output_loss: 1.1695 - val_loss: 0.7170 - val_main_output_loss: 0.4604 - val_aux_output_loss: 3.0262\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.5266 - main_output_loss: 0.4591 - aux_output_loss: 1.1344 - val_loss: 0.6510 - val_main_output_loss: 0.4293 - val_aux_output_loss: 2.6468\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.5173 - main_output_loss: 0.4520 - aux_output_loss: 1.1048 - val_loss: 0.6051 - val_main_output_loss: 0.4310 - val_aux_output_loss: 2.1722\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.5095 - main_output_loss: 0.4465 - aux_output_loss: 1.0765 - val_loss: 0.5644 - val_main_output_loss: 0.4161 - val_aux_output_loss: 1.8992\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.5027 - main_output_loss: 0.4417 - aux_output_loss: 1.0511 - val_loss: 0.5354 - val_main_output_loss: 0.4119 - val_aux_output_loss: 1.6466\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.4967 - main_output_loss: 0.4376 - aux_output_loss: 1.0280 - val_loss: 0.5124 - val_main_output_loss: 0.4047 - val_aux_output_loss: 1.4812\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 730us/step - loss: 0.4916 - main_output_loss: 0.4343 - aux_output_loss: 1.0070 - val_loss: 0.4934 - val_main_output_loss: 0.4034 - val_aux_output_loss: 1.3035\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.4867 - main_output_loss: 0.4311 - aux_output_loss: 0.9872 - val_loss: 0.4801 - val_main_output_loss: 0.3984 - val_aux_output_loss: 1.2150\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.4829 - main_output_loss: 0.4289 - aux_output_loss: 0.9686 - val_loss: 0.4694 - val_main_output_loss: 0.3962 - val_aux_output_loss: 1.1279\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 722us/step - loss: 0.4785 - main_output_loss: 0.4260 - aux_output_loss: 0.9510 - val_loss: 0.4580 - val_main_output_loss: 0.3936 - val_aux_output_loss: 1.0372\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.4756 - main_output_loss: 0.4246 - aux_output_loss: 0.9344 - val_loss: 0.4655 - val_main_output_loss: 0.4048 - val_aux_output_loss: 1.0118\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando evaluamos el modelo, Keras devolverá la pérdida total, así como las pérdidas individuales. De forma similar, el método `predict()` devolverá las predicciones para cada salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 506us/step - loss: 0.4668 - main_output_loss: 0.4178 - aux_output_loss: 0.9082\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C28DC8DE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26762432],\n",
       "       [1.9807628 ],\n",
       "       [3.3396287 ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9593649],\n",
       "       [1.9240992],\n",
       "       [2.5152814]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, podemos construir cualquier tipo de arquitectura que queramos de forma bastante fácil con la API Funcional. Echemos un vistazo a la última forma en que podemos construir modelos de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo modelos dinámicos usando la API Subclases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto la API Secuencial como la APU Funcional son declarativas: empezamos declarando cuántas capas queremos usar y cómo deben conectarse, y solo entonces podemos empezar a alimentar el modelo con datos para entrenamiento o inferencia. Esto tiene muchas ventajas: el modelo puede ser salvado, clonado y compartido fácilmente; su estructura se puede mostrar y analizar, el framework puede inferir formas y comprobar tipos, así los errores se pueden detectar con mayor anticipación (es decir, antes de que los datos pasen por el modelo). También es bastante fácil de depurar, dado que todo el modelo completo es solo un gráfico estático de capas. Pero la otra cara es precisamente esa: es estático. Algunos modelos incluyen bucles, formas variables, ramificaciones condicionales y otros tipos de comportamiento dinámico. Para tales casos, o simplemente si preferimos un estilo de programación más imperativo, la API Subclases está para nosotros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplemente creamos una subclase de la clase `Model`, creamos las capas que necesitamos en el constructor y la usamos para ejecutar los cálculos que queremos en el método `call()`. Por ejemplo, creando una instancia de la clase siguiente `WideAndDeepModel` obtenemos un modelo equivalente al que hemos construido con la API Funcional. Entonces podemos compilarlo, evaluarlo y usarlo para hacer predicciones, exactamente como hicimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4718 - main_output_loss: 0.4222 - aux_output_loss: 0.9186 - val_loss: 0.4452 - val_main_output_loss: 0.3892 - val_aux_output_loss: 0.9487\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 730us/step - loss: 0.4690 - main_output_loss: 0.4208 - aux_output_loss: 0.9028 - val_loss: 0.4523 - val_main_output_loss: 0.3997 - val_aux_output_loss: 0.9252\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.4662 - main_output_loss: 0.4194 - aux_output_loss: 0.8881 - val_loss: 0.4416 - val_main_output_loss: 0.3915 - val_aux_output_loss: 0.8931\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.4627 - main_output_loss: 0.4169 - aux_output_loss: 0.8748 - val_loss: 0.4422 - val_main_output_loss: 0.3948 - val_aux_output_loss: 0.8696\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.4608 - main_output_loss: 0.4164 - aux_output_loss: 0.8609 - val_loss: 0.4301 - val_main_output_loss: 0.3835 - val_aux_output_loss: 0.8495\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 755us/step - loss: 0.4574 - main_output_loss: 0.4139 - aux_output_loss: 0.8488 - val_loss: 0.4383 - val_main_output_loss: 0.3939 - val_aux_output_loss: 0.8374\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 752us/step - loss: 0.4555 - main_output_loss: 0.4132 - aux_output_loss: 0.8362 - val_loss: 0.4281 - val_main_output_loss: 0.3841 - val_aux_output_loss: 0.8240\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.4532 - main_output_loss: 0.4118 - aux_output_loss: 0.8250 - val_loss: 0.4229 - val_main_output_loss: 0.3791 - val_aux_output_loss: 0.8171\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 733us/step - loss: 0.4504 - main_output_loss: 0.4100 - aux_output_loss: 0.8141 - val_loss: 0.4293 - val_main_output_loss: 0.3876 - val_aux_output_loss: 0.8041\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.4483 - main_output_loss: 0.4089 - aux_output_loss: 0.8037 - val_loss: 0.4251 - val_main_output_loss: 0.3838 - val_aux_output_loss: 0.7964\n",
      "162/162 [==============================] - 0s 506us/step - loss: 0.4412 - main_output_loss: 0.4036 - aux_output_loss: 0.7798\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C2A80B4040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo se parece mucho a la API Funcional, excepto que no necesitamos crear las entradas, solo usamos el argumento `input` para el método `call()` y separamos la creación de las capas en el constructor de su uso en el método `call()` (*Los modelos de Keras tienen una atributo `output`, por tanto no podemos utilizar este nombre para la capa de salida principal, que es por lo que la renombramos a `main_output`*). Sin embargo, la gran diferencia es que podemos hacer prácticamente todo lo que queramos en el método `call()`: bucles `for`, sentencias `if`, operaciones de TensorFlow de bajo nivel, nuestro límite es nuestra imaginación. Esto la hace una fantástica API para que los investigadores experimenten con nuevas ideas.\n",
    "\n",
    "Sin embargo, esta flexibilidad extra viene con un coste: la arquitectura del modelo está oculta dentro del método `call()`, por lo que Keras no puede inspeccionarlo fácilmente, no puede salvarlo o clonarlo, y cuando llamamos al método `summary()` solo obtendremos una lista de capas, sin ninguna información de cómo están conectadas. Además, Keras no puede comprobar tipos ni formas antes de tiempo y es fácil cometer errores. Por tanto, a menos que necesitemos realmente esta flexibilidad extra, deberíamos ceñirnos a la API Secuencia o Funcional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Los modelos de Keras pueden usarse como capas regulares, por lo que podemos combinarlos fácilmente para construir arquitecturas complejas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos cómo construir y entrenar redes neuronales usando Keras, queremos salvarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar y restaurar un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar un modelo entrenado de Keras es tan simple como esto:\n",
    "\n",
    "    model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 837us/step - loss: 1.8866 - val_loss: 0.7126\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.6577 - val_loss: 0.6880\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.5934 - val_loss: 0.5803\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.5557 - val_loss: 0.5166\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.5272 - val_loss: 0.4895\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5033 - val_loss: 0.4951\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4854 - val_loss: 0.4861\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.4709 - val_loss: 0.4554\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.4578 - val_loss: 0.4413\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.4474 - val_loss: 0.4379\n",
      "162/162 [==============================] - 0s 395us/step - loss: 0.4382\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras guardará tanto la arquitectura del modelo (incluyendo los hiperparámetros de cada capa) como los valores de todos los parámetros del modelo de cada capa (es decir, pesos de conexión y sesgos), usando el formato HDF5. También guarda el optimizador (incluyendo sus hiperparámetros y cualquier estado que puedan tener).\n",
    "\n",
    "Normalmente tendremos un script que entrena un modelo y lo guarda, y uno o más scripts (o servicios web) que cargan el modelo y lo usan para hacer predicciones. Cargar el modelo es tan fácil como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C28D5C6280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5400236],\n",
       "       [1.6505969],\n",
       "       [3.0098243]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Esto funcionará bien cuando usemos API Secuencial o Funcional, pero desafortunadamente no cuando usemos subclases de Model. Sin embargo, podemos usar **`save_weights()`** y **`load_weights()`** para al menos guardar y restaurar los parámetros del modelo (pero deberemos guardar y restaurar todo lo demás por nosotros mismos).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c28d7d7790>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, ¿y si el entrenamiento dura varias horas? Esto es muy común, especialmente cuando entrenamos datasets grandes. En este caso, no solo debemos salvar nuestro modelo al final del entrenamiento sino también guardar puntos de control a intervalos regulares durante el entrenamiento. Pero, ¿cómo podemos decirle al método `fit()` que guarde puntos de control? La respuesta es usando callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `fit()` acepta un argumento `callbacks` que nos permite especificar una lista de objetos a los que Keras llamará durante el entrenamiento, al principio y al final del mismo, y al principio y al final de cada ciclo e incluso antes y después de procesar cada lote. Por ejemplo, el callback `ModelCheckpoint` guarda los puntos de control de nuestro modelo a intervales regulares durante el entrenamiento, por defecto al final de cada ciclo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, si usamos un conjunto de validación durante el entrenamiento, podemos establecer `save_best_only=True` cuando creamos el `ModelCheckpoint`. En este caso, solo guardará nuestro modelo cuando su ejecución en el conjunto de validación sea el mejor hasta el momento. De esta forma, no necesitamos preocuparnos porque el entrenamiento dure demasiado y sobreajustar el conjunto de entrenamiento: simplemente restauramos el último modelo guardado tras el entrenamiento y este será el mejor modelo en el conjunto de validación. Esta es una manera sencilla de implementar detención temprana (presentada en el capítulo 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 854us/step - loss: 1.8866 - val_loss: 0.7126\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.6577 - val_loss: 0.6880\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.5934 - val_loss: 0.5803\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.5557 - val_loss: 0.5166\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.5272 - val_loss: 0.4895\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.5033 - val_loss: 0.4951\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.4854 - val_loss: 0.4861\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.4709 - val_loss: 0.4554\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.4578 - val_loss: 0.4413\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.4474 - val_loss: 0.4379\n",
      "162/162 [==============================] - 0s 395us/step - loss: 0.4382\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", \n",
    "                                                save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de implementar la detención temprana es utilizar simplemente el callback `EarlyStopping`. Interrumpirá el entrenamiento cuando no detecte progreso en el conjunto de validación durante un número de ciclos (definido por el argumento `patience`) y, opcionalmente, volverá al mejor modelo. Podemos combinar ambos callbacks para guardar checkpoints de nuestro modelo (en el caso de que nuesto ordenador sufra una caida del sistema) e interrumpir el entrenamiento prematuramente cuando no existe más progreso (evitando gastar tiempo y recursos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.4393 - val_loss: 0.4110\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.4315 - val_loss: 0.4266\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4259 - val_loss: 0.3996\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.4201 - val_loss: 0.3939\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.4154 - val_loss: 0.3889\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.4111 - val_loss: 0.3866\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.4074 - val_loss: 0.3860\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.4040 - val_loss: 0.3793\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.4008 - val_loss: 0.3746\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3976 - val_loss: 0.3723\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3950 - val_loss: 0.3697\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.3923 - val_loss: 0.3669\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.3897 - val_loss: 0.3661\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.3874 - val_loss: 0.3631\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.3851 - val_loss: 0.3660\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3829 - val_loss: 0.3625\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.3810 - val_loss: 0.3592\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.3788 - val_loss: 0.3563\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3766 - val_loss: 0.3535\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.3750 - val_loss: 0.3709\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.3732 - val_loss: 0.3512\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3715 - val_loss: 0.3699\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.3700 - val_loss: 0.3476\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3685 - val_loss: 0.3561\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3671 - val_loss: 0.3527\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3658 - val_loss: 0.3700\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.3647 - val_loss: 0.3432\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3635 - val_loss: 0.3592\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 603us/step - loss: 0.3625 - val_loss: 0.3521\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 603us/step - loss: 0.3613 - val_loss: 0.3626\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3601 - val_loss: 0.3431\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.3589 - val_loss: 0.3765\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.3584 - val_loss: 0.3374\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3572 - val_loss: 0.3407\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3563 - val_loss: 0.3614\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.3555 - val_loss: 0.3348\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3546 - val_loss: 0.3573\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3538 - val_loss: 0.3367\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3530 - val_loss: 0.3425\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3523 - val_loss: 0.3369\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3515 - val_loss: 0.3515\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3511 - val_loss: 0.3426\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3500 - val_loss: 0.3677\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3496 - val_loss: 0.3564\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3490 - val_loss: 0.3336\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.3481 - val_loss: 0.3457\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3478 - val_loss: 0.3433\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3471 - val_loss: 0.3659\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.3466 - val_loss: 0.3286\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.3460 - val_loss: 0.3268\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3454 - val_loss: 0.3439\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.3449 - val_loss: 0.3263\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3444 - val_loss: 0.3910\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3439 - val_loss: 0.3275\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3435 - val_loss: 0.3561\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.3430 - val_loss: 0.3237\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.3423 - val_loss: 0.3242\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.3419 - val_loss: 0.3765\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.3417 - val_loss: 0.3289\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3410 - val_loss: 0.3502\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.3404 - val_loss: 0.3456\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3402 - val_loss: 0.3445\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3392 - val_loss: 0.3290\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.3393 - val_loss: 0.3217\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3387 - val_loss: 0.3351\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3383 - val_loss: 0.3232\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.3376 - val_loss: 0.3566\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3374 - val_loss: 0.3257\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3370 - val_loss: 0.3348\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.3365 - val_loss: 0.3560\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3361 - val_loss: 0.3583\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3357 - val_loss: 0.3287\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.3351 - val_loss: 0.3203\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.3350 - val_loss: 0.3840\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.3347 - val_loss: 0.3233\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3342 - val_loss: 0.3476\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3338 - val_loss: 0.3407\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.3335 - val_loss: 0.3462\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3332 - val_loss: 0.3347\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3329 - val_loss: 0.3354\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3324 - val_loss: 0.3274\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3320 - val_loss: 0.3167\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.3317 - val_loss: 0.3280\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.3312 - val_loss: 0.3634\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3310 - val_loss: 0.3176\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.3308 - val_loss: 0.3156\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3305 - val_loss: 0.3529\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3299 - val_loss: 0.3258\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3294 - val_loss: 0.3630\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3296 - val_loss: 0.3376\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3291 - val_loss: 0.3211\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3287 - val_loss: 0.3456\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3285 - val_loss: 0.3158\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3281 - val_loss: 0.3409\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3276 - val_loss: 0.3379\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.3273 - val_loss: 0.3213\n",
      "162/162 [==============================] - 0s 420us/step - loss: 0.3310\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de ciclos se puede establecer a un valor alto, dado que el entrenamiento se detendrá automáticamente cuando no haya progreso. En este caso, no es necesario restaurar el mejor modelo guardado puesto que el callback `EarlyStopping` mantendrá los mejores pesos y los restaurará por nosotros al final del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Existe muchos otros callbacks disponibles en el [`paquete keras.callbacks`](https://keras.io/callbacks/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si necesitamos un control extra, podemos escribir fácilmente nuestros propios callbacks personalizados. Como ejemplo de cómo hacer esto, el siguiente callback personalizado mostrará el ratio entre la pérdida de validación y la pérdida de entrenamiento durante el entrenamiento (por ejemplo, para detectar el sobreajuste):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/363 [=========================>....] - ETA: 0s - loss: 0.3296\n",
      "val/train: 1.08\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.3302 - val_loss: 0.3556\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cabría esperar, podemos implementar `on_train_begin()`, `on_train_end()`, `on_epoch_begin()`, `on_epoch_end()`, `on_batch_begin()` y `on_batch_end()`. Los callbacks también se pueden usar durante la evaluación y predicción, en el caso de que los necesites. Para la evaluación debemos implementar `on_test_begin()`, `on_test_end()`, `on_test_batch_begin()` o `on_test_batch_end()` (llamados por `evaluate()`); y para la predicción debemos implementar `on_predict_begin()`, `on_predict_end()`, `on_predict_batch_begin()` o `on_predict_batch_end()` (llamados por `predict()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora echemos un vistazo a una herramienta más que sin lugar a dudas deberíamos tener en nuestra caja de herramientas cuando usemos tf.keras: TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de TensorBoard para visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard es una gran herramienta de visualización interactiva que podemos usar para visualizar las curvas de aprendizaje durante el entrenamiento, comparar curvas de aprendizaje entre múltiples ejecuciones, visualizar el gráfico de cálculo, analizar estadísticas de entrenamiento, visualizar imágenes generadas por nuestro modelo, visualizar datos multidimensionales complejos proyectados en 3D y clusterizados automáticamente para nosotros, etc. Esta herramienta se instala automáticamente al instalar TensorFlow, por lo que ya la tenemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizarla, debemos modificar el programa que devuelve la salida de los datos que queremos visualizar a unos archivos de log binarios especiales denominados *archivos de eventos*. Cada registro de datos binarios se denomina un *resumen* (*summary*). El servidor de TensorBoard monitorizará el directorio de log y detectará automáticamente los cambios y actualizará las visualizaciones: esto permite visualizar datos en tiempo real (con un pequeño retardo), tales como curvas de aprendizaje durante el entrenamiento. En general, apuntaremos el servidor de TensorBoard a un directorio raíz de logs y configuraremos nuestro programa para escribir a un subdirectorio diferente cada vez que se ejecuta. De esta forma, la misma instancia de servidor de TensorBoard permitirá visualizar y comparar datos entre múltiples ejecuciones de nuestro programa, sin tenerlo todo mezclado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos definiendo el directorio raíz de logs que usaremos para nuestros logs de TensorBoard, más una pequeña función que generará un path de subdirectorio basado en la fecha y hora actuales, para que sea diferente en cada ejecución. También podríamos incluir información extra en el nombre del directorio de log, tales como valores de hiperparámetros que estamos probando, para hacer más fácil saber lo que estamos viendo en TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2020_08_27-13_37_02'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las buenas noticias es que Keras proporciona un bonito callback `TensorBoard()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/363 [..............................] - ETA: 0s - loss: 7.8215WARNING:tensorflow:From C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/363 [..............................] - ETA: 11s - loss: 7.0195WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_begin` time: 0.0030s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0590s). Check your callbacks.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.8866 - val_loss: 0.7126\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.6577 - val_loss: 0.6880\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5934 - val_loss: 0.5803\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.5557 - val_loss: 0.5166\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.5272 - val_loss: 0.4895\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5033 - val_loss: 0.4951\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.4854 - val_loss: 0.4861\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.4709 - val_loss: 0.4554\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.4578 - val_loss: 0.4413\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.4474 - val_loss: 0.4379\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.4393 - val_loss: 0.4396\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.4318 - val_loss: 0.4507\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.4261 - val_loss: 0.3997\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4202 - val_loss: 0.3956\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.4155 - val_loss: 0.3916\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.4112 - val_loss: 0.3937\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4077 - val_loss: 0.3809\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4040 - val_loss: 0.3793\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 637us/step - loss: 0.4004 - val_loss: 0.3850\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 615us/step - loss: 0.3980 - val_loss: 0.3809\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.3949 - val_loss: 0.3701\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.3924 - val_loss: 0.3781\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3898 - val_loss: 0.3650\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.3874 - val_loss: 0.3655\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3851 - val_loss: 0.3611\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3829 - val_loss: 0.3626\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.3809 - val_loss: 0.3564\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3788 - val_loss: 0.3579\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3769 - val_loss: 0.3561\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3750 - val_loss: 0.3548\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Y eso es todo lo que hay que hacer! No puede ser más sencillo. Al ejecutar este código, el callback `TensorBoard()` se encargará de crear el directorio de log por nosotros y durante el entrenamiento creará los ficheros de eventos y escribirá los resúmenes en ellos. Tras ejecutar el programa una segunda vez (quizás cambiando algunos valores de hiperparámetros), terminaremos con una estructura de directorios simiar a esta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![directory_structure](images/ch10/directory_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe un directorio por ejecución, cada uno conteniendo un subdirectorio para los logs de entrenamiento y otro para los de validación. Ambos contiene ficheros de eventos, pero los logs de entrenamiento también incluyen trazas que permiten a TensorBoard mostrarnos exactamente cuánto tiempo dedica el modelo en cada parte de nuestro modelo, a través de todos los disposivos, lo cual está fenomenal para localizar cuellos de botella en el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente que necesitamos es iniciar el servidor de TensorBoard. Una forma de hacer esto es ejecutar un comando un terminal. Si hemos instalado TensorFlow dentro de un virtualenv, debermos activarlo. Después, ejecutamos el siguiente comando en la raíz del proyecto (o desde cualquier otro sitio siempre que apuntemos al directorio de log adecuado):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ tensorboard --logdir=./my_logs --port=6006\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TensorBoard 2.3.0 at http://localhost:6006/ (Press CTRL+C to quit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el shell no puede encontrar el script *tensorboard*, deberemos cambiar nuestra variable de entorno PATH que contiene el directorio en el que se ha instalado el script (alternativamente, podemos reemplazar `tensorboard` en la línea de comando por `python -m tensorboard.main`). Una vez que se ha levantado el servidor, pordemos abrir un navegador web y acceder a [http://localhost:6006](http://localhost:6006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos usar TensorBoard directamente desde Jupyter, ejecutando los siguiente comandos. La primera línea carga la extensión de TensorBoard y la segunda inicia el servidor de TensorBoard en el puerto 6006 (a menos que ya esté iniciado) y lo conecta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13344), started 1:52:21 ago. (Use '!kill 13344' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-327d4ef08031008a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-327d4ef08031008a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2020_08_27-13_37_09'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir2 = get_run_logdir()\n",
    "run_logdir2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/363 [..............................] - ETA: 1:28 - loss: 5.0901WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0009s vs `on_train_batch_begin` time: 0.0021s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_train_batch_end` time: 0.4880s). Check your callbacks.\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5530 - val_loss: 302.8536\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 675us/step - loss: 5292745216.0000 - val_loss: 1.3230\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 1.3411 - val_loss: 1.3176\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 656us/step - loss: 1.3423 - val_loss: 1.3261\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 1.3423 - val_loss: 1.3154\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 636us/step - loss: 1.3431 - val_loss: 1.3203\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 620us/step - loss: 1.3425 - val_loss: 1.3149\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 1.3433 - val_loss: 1.3157\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 656us/step - loss: 1.3435 - val_loss: 1.3150\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 647us/step - loss: 1.3423 - val_loss: 1.3172\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 653us/step - loss: 1.3432 - val_loss: 1.3174\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 634us/step - loss: 1.3426 - val_loss: 1.3150\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 617us/step - loss: 1.3422 - val_loss: 1.3270\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 626us/step - loss: 1.3430 - val_loss: 1.3195\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 636us/step - loss: 1.3426 - val_loss: 1.3157\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 625us/step - loss: 1.3422 - val_loss: 1.3182\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 634us/step - loss: 1.3429 - val_loss: 1.3223\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 656us/step - loss: 1.3422 - val_loss: 1.3154\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 1.3421 - val_loss: 1.3168\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 1.3430 - val_loss: 1.3151\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 653us/step - loss: 1.3418 - val_loss: 1.3174\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 628us/step - loss: 1.3424 - val_loss: 1.3204\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 617us/step - loss: 1.3420 - val_loss: 1.3164\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 645us/step - loss: 1.3429 - val_loss: 1.3157\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 667us/step - loss: 1.3422 - val_loss: 1.3180\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 628us/step - loss: 1.3425 - val_loss: 1.3195\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 631us/step - loss: 1.3422 - val_loss: 1.3157\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 631us/step - loss: 1.3425 - val_loss: 1.3222\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 614us/step - loss: 1.3431 - val_loss: 1.3267\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 620us/step - loss: 1.3424 - val_loss: 1.3174\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De cualquier de las formas deberíamos ver el interfaz web de TensorBoard. Pulsa en la pestaña SCALARS para ver las curvas de aprendizaje (ver la siguiente figura). En la parte inferior izquierda, selecciona los logs queremos visualizar (por ejemplo, los logs de entrenamiento de la primera y segunda ejecución) y selecciona el escalar `epoch_loss`. Vemos que la pérdida de entrenamiento disminuyó durante ambos ejecuciones, pero en la segunda ejecución fue mucho más rápida. De hecho, usamos una tasa de aprendizaje de 0.05 (`optimizer=keras.optimizer.SGD(lr=0.05)`), en lugar de 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![curves_tb](images/ch10/curves_tb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos visializar el grafo completo, los pesos aprendidos (proyectados en 3D) o las trazas. El callback `TensorBoard()` tiene opciones de datos extra en los logs.\n",
    "\n",
    "Además, TensorFlow ofrece una API de bajo nivel en el paquete `tf.summary`. El código siguiente crear un `SummaryWriter` usando la función `create_file_writer()`, y usar este writer como un contexto para registrar escalares, histogramas, imágenes, audios y texto, todo aquellos que podamos visualizar usando TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images\n",
    "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad, esta es una herramienta de visualización útil a tener, incluso más allá de TensorFlow o Deep Learning.\n",
    "\n",
    "Vamos a resumir lo que hemos aprendido hasta ahora en este capítulo: hemos visto de dónde vienen las redes neuronales, lo que es un perceptrón multicapa (MLP) y cómo se puede usar para clasificación y regresión, cómo usar la API Secuencial de tf.keras para construir MLPs y cómo usar las API Funcional y API Subclases para construir arquitecturas de modelos complejas. Hemos aprendido cómo guardar  y restaurar un modelo y cómo usar callbacks para realizar checkpoints, detención temprana, etc. Por último, hemos aprendido cómo usar TensorBoard para visualización. Ya estamos listos para seguir adelante y usar redes neuronales para abordar muchos problemas. Sin embargo, es posible que nos preguntemos cómo elegir el número adecuado de capas ocultas, el número de neuronas de la red y el resto de hiperparámetros. Veamos esto ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de hiperparámetros de redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La flexibilidad de las redes neuronales también es uno de sus inconvenientes: hay muchos hiperparámetros que ajustar. No solo podemos usar cualquier arquitectura de red imaginables, sino que incluso en un simple MLP podemos cambiar el número de capas, el número de neuronas por capa, el tipo de función de activación a utilizar en cada capa, la lógica de inicialización de pesos, etc. ¿Cómo podemos saber qué combinación de hiperparámetros en la mejor para nuestra tarea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una opción es simplemente probar muchas combinaciones de hiperparámetros y ver cuáles funcionan mejor en el conjunto de validación (o usar validación cruzada K-fold). Por ejemplo, podemos usar `GridSearchCV` o `RandomizedSearchCV` para explorar el espacio de hiperparámetros, como hicimos en el capítulo 2. Para hacer esto, necesitamos envolver nuestro modelos de Keras en objetos que imiten los regresones normales de Scikit-Learn. El primer paso es crear una función que construirá y compilará un modelo de Keras, dados un conjunto de hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función crea un modelo `Sequential` simple para regresión univariable (solo una neurona de salida), con la forma de entrada dada y dado el número de capas ocultas y neuronas; y lo compila usando un optimizador SGD con la tasa de aprendizaje especificada. Es una buena práctica proporcionar tantos valores por defectos razonables como se pueda para la mayoría de los hiperparámetros, como hace Scikit-Learn.\n",
    "\n",
    "Después, vamos a crear un `KerasRegressor` basado en esta función `build_model()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto `KerasRegressor` es un pequeño envoltorio (wrapper) alrededor del modelo de Keras construido usando `build_model()`. Dado que no especificamos ningún hiperparámetros cuando lo creamos, usará los hiperparámetros por defecto definidos en `build_model()`. Ahora podemos usar este objeto como un regresor regular de Scikit-Learn: podemos entrenarlo usando el método `fit()`, después evaluarlo usando su método `score()` y usarlo para hacer predicciones usando su método `predict()`, como podemos ver en el código siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 1.0896 - val_loss: 20.7721\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.7606 - val_loss: 5.0266\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.5456 - val_loss: 0.5490\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4732 - val_loss: 0.4529\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4503 - val_loss: 0.4188\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.4338 - val_loss: 0.4129\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4241 - val_loss: 0.4004\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.4168 - val_loss: 0.3944\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4108 - val_loss: 0.3961\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.4060 - val_loss: 0.4071\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4021 - val_loss: 0.3855\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3984 - val_loss: 0.4136\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3951 - val_loss: 0.3997\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3921 - val_loss: 0.3818\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3894 - val_loss: 0.3829\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3869 - val_loss: 0.3739\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.3848 - val_loss: 0.4022\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3829 - val_loss: 0.3873\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3807 - val_loss: 0.3768\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.3791 - val_loss: 0.4191\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3774 - val_loss: 0.3927\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3756 - val_loss: 0.4237\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3742 - val_loss: 0.3523\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3725 - val_loss: 0.3842\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.3710 - val_loss: 0.4162\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3700 - val_loss: 0.3980\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3691 - val_loss: 0.3474\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.3677 - val_loss: 0.3920\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3670 - val_loss: 0.3566\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3653 - val_loss: 0.4191\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3647 - val_loss: 0.3721\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.3633 - val_loss: 0.3948\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3632 - val_loss: 0.3423\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.3617 - val_loss: 0.3453\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3610 - val_loss: 0.4068\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3608 - val_loss: 0.3417\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3596 - val_loss: 0.3787\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3589 - val_loss: 0.3379\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.3582 - val_loss: 0.3419\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.3572 - val_loss: 0.3705\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3570 - val_loss: 0.3659\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3563 - val_loss: 0.3803\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3552 - val_loss: 0.3765\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.3548 - val_loss: 0.3814\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3543 - val_loss: 0.3326\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.3532 - val_loss: 0.3385\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3527 - val_loss: 0.3655\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3521 - val_loss: 0.3579\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3525 - val_loss: 0.3360\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.3510 - val_loss: 0.3318\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.3504 - val_loss: 0.3562\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3502 - val_loss: 0.3520\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3496 - val_loss: 0.4579\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 559us/step - loss: 0.3497 - val_loss: 0.3808\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3490 - val_loss: 0.3539\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.3485 - val_loss: 0.3723\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3479 - val_loss: 0.3336\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.3469 - val_loss: 0.4011\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3475 - val_loss: 0.3264\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3465 - val_loss: 0.3271\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3452 - val_loss: 0.3346\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3453 - val_loss: 0.3493\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 556us/step - loss: 0.3444 - val_loss: 0.3402\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3450 - val_loss: 0.3275\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3437 - val_loss: 0.3296\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3431 - val_loss: 0.3307\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3428 - val_loss: 0.3252\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3423 - val_loss: 0.3242\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3419 - val_loss: 0.3254\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3413 - val_loss: 0.3672\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.3414 - val_loss: 0.3375\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3405 - val_loss: 0.3271\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.3399 - val_loss: 0.3242\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.3402 - val_loss: 0.3665\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3397 - val_loss: 0.3283\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3395 - val_loss: 0.3240\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3383 - val_loss: 0.3381\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3384 - val_loss: 0.3356\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3383 - val_loss: 0.3224\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3376 - val_loss: 0.3595\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3383 - val_loss: 0.3432\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3371 - val_loss: 0.3211\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3367 - val_loss: 0.3342\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3362 - val_loss: 0.4136\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3369 - val_loss: 0.3285\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3359 - val_loss: 0.3440\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.3357 - val_loss: 0.3733\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3355 - val_loss: 0.3188\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3346 - val_loss: 0.3492\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.3348 - val_loss: 0.3175\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3339 - val_loss: 0.3594\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3342 - val_loss: 0.3169\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3333 - val_loss: 0.3607\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3344 - val_loss: 0.5184\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3339 - val_loss: 0.7536\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3370 - val_loss: 0.5075\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3340 - val_loss: 0.8087\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3354 - val_loss: 1.0447\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3346 - val_loss: 1.6881\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.3495 - val_loss: 1.9265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c28c1d30d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/162 [..............................] - ETA: 0s - loss: 0.2524WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "162/162 [==============================] - 0s 389us/step - loss: 0.3409\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C28AA5F040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengamos en cuenta que cualquier parámetro extra que pasemos al método `fit()` se pasará al modelo de Keras subyacente. También tengamos en cuenta que la puntuación será la opuesta al MSE debido a que Scikit-Learn quiere puntuaciones, no pérdidas (es decir, más alto es mejor).\n",
    "\n",
    "No queremos entrenar y evaluar un único modelo como este, sino entrenar cientos de variantes y ver cuáles se ejecutan mejor en el conjunto de validación. Dado que existen muchos hiperparámetros, es preferible usar búsqueda aleatoria en lugar de grid search (como vimos en el capítulo 2). Vamos a intentar explorar el número de capas ocultas, el número de neuronas y la tasa de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 971us/step - loss: 3.5557 - val_loss: 1.8752\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.3347 - val_loss: 0.9522\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.8591 - val_loss: 0.7820\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7360 - val_loss: 0.7249\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6930 - val_loss: 0.6994\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.6668 - val_loss: 0.9118\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.6514 - val_loss: 0.8495\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6381 - val_loss: 0.8605\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6276 - val_loss: 0.6524\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6125 - val_loss: 0.8619\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.6057 - val_loss: 0.8659\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.5993 - val_loss: 0.5962\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5859 - val_loss: 0.9062\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.5828 - val_loss: 0.9541\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.5799 - val_loss: 0.6402\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5706 - val_loss: 0.7806\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.5670 - val_loss: 0.7985\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5620 - val_loss: 0.8756\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5585 - val_loss: 0.8958\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5564 - val_loss: 0.8657\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5559 - val_loss: 0.5940\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.5476 - val_loss: 0.8007\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5484 - val_loss: 0.7792\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.5459 - val_loss: 0.7622\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.5453 - val_loss: 0.6476\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5431 - val_loss: 0.5424\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.5373 - val_loss: 0.8687\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.5424 - val_loss: 0.5390\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5365 - val_loss: 0.7179\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5384 - val_loss: 0.6029\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.5362 - val_loss: 0.5947\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5359 - val_loss: 0.5305\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.5334 - val_loss: 0.6601\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5341 - val_loss: 0.6326\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5344 - val_loss: 0.5072\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5304 - val_loss: 0.7270\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5341 - val_loss: 0.5055\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5284 - val_loss: 0.7985\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5338 - val_loss: 0.5176\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5305 - val_loss: 0.5823\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.5293 - val_loss: 0.7114\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5322 - val_loss: 0.5059\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5302 - val_loss: 0.5008\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.5274 - val_loss: 0.7397\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5309 - val_loss: 0.6169\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.5303 - val_loss: 0.5264\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5276 - val_loss: 0.6916\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5298 - val_loss: 0.6554\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5290 - val_loss: 0.6607\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5267 - val_loss: 0.8497\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5310 - val_loss: 0.6664\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.5294 - val_loss: 0.5996\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5282 - val_loss: 0.6414\n",
      "121/121 [==============================] - 0s 364us/step - loss: 0.5368\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=   8.6s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 930us/step - loss: 3.5605 - val_loss: 23.0855\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 1.4777 - val_loss: 10.8387\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 1.0149 - val_loss: 4.4392\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.8729 - val_loss: 1.5338\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.8027 - val_loss: 0.7192\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.7542 - val_loss: 1.2046\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.7160 - val_loss: 2.4524\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.6847 - val_loss: 4.1421\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6588 - val_loss: 5.9820\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6371 - val_loss: 7.7654\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.6187 - val_loss: 9.6230\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.6029 - val_loss: 11.3609\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5896 - val_loss: 12.9821\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5781 - val_loss: 14.2266\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5683 - val_loss: 15.4321\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.9198\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=   2.7s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 3.2972 - val_loss: 1.3307\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.9648 - val_loss: 0.6934\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.6150 - val_loss: 0.5469\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5468 - val_loss: 0.7322\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5372 - val_loss: 0.4963\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.5330 - val_loss: 0.5539\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5320 - val_loss: 0.5729\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.5297 - val_loss: 0.7873\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5337 - val_loss: 0.5968\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5314 - val_loss: 0.4951\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5286 - val_loss: 0.7591\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5333 - val_loss: 0.5368\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.5305 - val_loss: 0.4968\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5305 - val_loss: 0.5778\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5313 - val_loss: 0.5117\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.5282 - val_loss: 0.7055\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5320 - val_loss: 0.5399\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.5307 - val_loss: 0.5257\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5275 - val_loss: 0.7902\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5327 - val_loss: 0.5852\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.5317\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=   3.5s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 4.8657WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "242/242 [==============================] - 0s 967us/step - loss: 1.4256 - val_loss: 66.5657\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.9941 - val_loss: 137.1489\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 2.2587 - val_loss: 716.1609\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 4.3545 - val_loss: 2297.8599\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 17.0750 - val_loss: 9988.3369\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 198.7058 - val_loss: 39231.9727\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 424.9946 - val_loss: 155196.9375\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 2992.7771 - val_loss: 612492.9375\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 7662.3345 - val_loss: 2435756.7500\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 53693.9609 - val_loss: 10128971.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 135638.0156 - val_loss: 39694556.0000\n",
      "121/121 [==============================] - 0s 380us/step - loss: 105477.5781\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   2.3s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 1.1573 - val_loss: 23.1193\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5349 - val_loss: 22.1675\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.5192 - val_loss: 22.3752\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5148 - val_loss: 21.3891\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5108 - val_loss: 20.8855\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5082 - val_loss: 20.6379\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5070 - val_loss: 20.0736\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5050 - val_loss: 20.7178\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.5029 - val_loss: 20.0844\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5032 - val_loss: 17.0622\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5036 - val_loss: 19.1666\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5017 - val_loss: 20.8246\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5023 - val_loss: 22.0298\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5049 - val_loss: 17.6022\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5024 - val_loss: 18.6171\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5024 - val_loss: 20.0451\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.5005 - val_loss: 17.5898\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.5040 - val_loss: 17.4526\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5012 - val_loss: 19.5015\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.5011 - val_loss: 17.3223\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.9327\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   3.5s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 1.4616 - val_loss: 0.5742\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.6113 - val_loss: 6.7367\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5784 - val_loss: 6.5227\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.5820 - val_loss: 19.7083\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.6738 - val_loss: 205.7215\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.6846 - val_loss: 282.6048\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 2.5718 - val_loss: 656.3251\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 12.3829 - val_loss: 1380.0117\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 14.8443 - val_loss: 2817.4534\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 7.4320 - val_loss: 4499.3799\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 599us/step - loss: 121.3308 - val_loss: 8457.8672\n",
      "121/121 [==============================] - 0s 364us/step - loss: 11.0521\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   2.1s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5089 - val_loss: 2.6033\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 1.0793 - val_loss: 1.0424\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.8038 - val_loss: 0.7507\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.7203 - val_loss: 0.6758\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.6785 - val_loss: 0.6484\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.6498 - val_loss: 0.6241\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.6261 - val_loss: 0.6073\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.6055 - val_loss: 0.5826\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.5870 - val_loss: 0.5597\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.5700 - val_loss: 0.5445\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.5547 - val_loss: 0.5314\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5408 - val_loss: 0.5147\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.5278 - val_loss: 0.5030\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5159 - val_loss: 0.4904\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5051 - val_loss: 0.4791\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4948 - val_loss: 0.4695\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4857 - val_loss: 0.4608\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4772 - val_loss: 0.4524\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4691 - val_loss: 0.4476\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4621 - val_loss: 0.4383\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4555 - val_loss: 0.4355\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4494 - val_loss: 0.4282\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4437 - val_loss: 0.4230\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4384 - val_loss: 0.4166\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4339 - val_loss: 0.4161\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4294 - val_loss: 0.4142\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4254 - val_loss: 0.4100\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4217 - val_loss: 0.4132\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.4180 - val_loss: 0.4103\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4149 - val_loss: 0.4032\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4118 - val_loss: 0.3964\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4089 - val_loss: 0.3956\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4062 - val_loss: 0.4013\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4035 - val_loss: 0.4004\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4012 - val_loss: 0.3913\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3988 - val_loss: 0.3986\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3968 - val_loss: 0.3871\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3946 - val_loss: 0.3998\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3927 - val_loss: 0.3858\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3908 - val_loss: 0.3967\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3890 - val_loss: 0.3918\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3873 - val_loss: 0.3866\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3858 - val_loss: 0.3800\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3840 - val_loss: 0.3997\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3827 - val_loss: 0.3861\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3811 - val_loss: 0.3805\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3799 - val_loss: 0.3919\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3785 - val_loss: 0.3826\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3772 - val_loss: 0.3812\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3759 - val_loss: 0.3905\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3746 - val_loss: 0.3832\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3737 - val_loss: 0.3827\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3725 - val_loss: 0.3859\n",
      "121/121 [==============================] - 0s 397us/step - loss: 0.3865\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  10.3s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 6.4054WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7762 - val_loss: 17.5435\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 1.1017 - val_loss: 15.4502\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.8039 - val_loss: 11.1084\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.7051 - val_loss: 8.0885\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.6575 - val_loss: 6.1076\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6260 - val_loss: 4.7302\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6007 - val_loss: 3.6783\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5790 - val_loss: 2.8274\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.5600 - val_loss: 2.2526\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.5433 - val_loss: 1.7966\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5284 - val_loss: 1.4646\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5153 - val_loss: 1.1656\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.5036 - val_loss: 0.9599\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4932 - val_loss: 0.8400\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4839 - val_loss: 0.7148\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4756 - val_loss: 0.6408\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4680 - val_loss: 0.5679\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4612 - val_loss: 0.5264\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4549 - val_loss: 0.4894\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4493 - val_loss: 0.4711\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4442 - val_loss: 0.4525\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.4395 - val_loss: 0.4467\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4352 - val_loss: 0.4404\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4313 - val_loss: 0.4333\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4276 - val_loss: 0.4302\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4242 - val_loss: 0.4284\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4209 - val_loss: 0.4270\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.4180 - val_loss: 0.4269\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4148 - val_loss: 0.4416\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4127 - val_loss: 0.4363\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4099 - val_loss: 0.4330\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4074 - val_loss: 0.4408\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4052 - val_loss: 0.4484\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4029 - val_loss: 0.4647\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4009 - val_loss: 0.4789\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.3989 - val_loss: 0.4746\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3970 - val_loss: 0.4974\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3951 - val_loss: 0.5135\n",
      "121/121 [==============================] - 0s 413us/step - loss: 0.4088\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=   7.5s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8501 - val_loss: 2.0961\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 1.1187 - val_loss: 1.2079\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.8431 - val_loss: 0.8075\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.7630 - val_loss: 0.7207\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.7220 - val_loss: 0.6952\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6925 - val_loss: 0.6614\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6677 - val_loss: 0.6378\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.6461 - val_loss: 0.6132\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6268 - val_loss: 0.6043\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6081 - val_loss: 0.5937\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.5908 - val_loss: 0.5658\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.5749 - val_loss: 0.5551\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.5601 - val_loss: 0.5476\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5465 - val_loss: 0.5450\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5340 - val_loss: 0.5314\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.5225 - val_loss: 0.5067\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5119 - val_loss: 0.4983\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.5020 - val_loss: 0.4873\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4930 - val_loss: 0.4748\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4849 - val_loss: 0.4767\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4771 - val_loss: 0.4719\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4701 - val_loss: 0.4623\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4637 - val_loss: 0.4640\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4575 - val_loss: 0.4777\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.4521 - val_loss: 0.4488\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4469 - val_loss: 0.4475\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4423 - val_loss: 0.4420\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4378 - val_loss: 0.4449\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4337 - val_loss: 0.4581\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4301 - val_loss: 0.4385\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.4265 - val_loss: 0.4226\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4233 - val_loss: 0.4458\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4201 - val_loss: 0.4242\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.4174 - val_loss: 0.4542\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4149 - val_loss: 0.4279\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4122 - val_loss: 0.4341\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4101 - val_loss: 0.4189\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4078 - val_loss: 0.4344\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4056 - val_loss: 0.4235\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4037 - val_loss: 0.4183\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4015 - val_loss: 0.4552\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4001 - val_loss: 0.4411\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3981 - val_loss: 0.4073\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3967 - val_loss: 0.4294\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3950 - val_loss: 0.4238\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3936 - val_loss: 0.4128\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3921 - val_loss: 0.3977\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3909 - val_loss: 0.4028\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3893 - val_loss: 0.4362\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3884 - val_loss: 0.4235\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3870 - val_loss: 0.4171\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3859 - val_loss: 0.4273\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3849 - val_loss: 0.4076\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3839 - val_loss: 0.3885\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3827 - val_loss: 0.4003\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3816 - val_loss: 0.4176\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3808 - val_loss: 0.4201\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3800 - val_loss: 0.4177\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3789 - val_loss: 0.4166\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3779 - val_loss: 0.3910\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3772 - val_loss: 0.4094\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3763 - val_loss: 0.4363\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3757 - val_loss: 0.4025\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3749 - val_loss: 0.4028\n",
      "121/121 [==============================] - 0s 421us/step - loss: 0.3737\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  12.3s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4720 - val_loss: 7.9722\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.1323 - val_loss: 5.6563\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.8832 - val_loss: 4.1443\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.8066 - val_loss: 3.1169\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.7657 - val_loss: 2.6199\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.7374 - val_loss: 2.2830\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.7171 - val_loss: 1.9726\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.6983 - val_loss: 1.7536\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.6816 - val_loss: 1.5653\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6663 - val_loss: 1.4316\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6521 - val_loss: 1.3165\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6387 - val_loss: 1.2101\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6261 - val_loss: 1.1236\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6139 - val_loss: 1.0591\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6024 - val_loss: 0.9875\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5910 - val_loss: 0.9345\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5804 - val_loss: 0.8832\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5701 - val_loss: 0.8424\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5602 - val_loss: 0.8079\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5511 - val_loss: 0.7646\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5420 - val_loss: 0.7347\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5332 - val_loss: 0.7075\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5249 - val_loss: 0.6815\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5165 - val_loss: 0.6537\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5095 - val_loss: 0.6360\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5020 - val_loss: 0.6174\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4952 - val_loss: 0.6010\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4887 - val_loss: 0.5887\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4824 - val_loss: 0.5778\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4766 - val_loss: 0.5671\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4710 - val_loss: 0.5557\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4658 - val_loss: 0.5475\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4608 - val_loss: 0.5403\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4561 - val_loss: 0.5321\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4516 - val_loss: 0.5250\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4476 - val_loss: 0.5165\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4436 - val_loss: 0.5106\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4400 - val_loss: 0.5053\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4364 - val_loss: 0.5004\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4333 - val_loss: 0.4966\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4302 - val_loss: 0.4922\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4274 - val_loss: 0.4891\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4248 - val_loss: 0.4850\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4220 - val_loss: 0.4854\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4198 - val_loss: 0.4828\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4173 - val_loss: 0.4779\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4153 - val_loss: 0.4783\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4131 - val_loss: 0.4755\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4111 - val_loss: 0.4766\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4093 - val_loss: 0.4753\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4074 - val_loss: 0.4714\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4060 - val_loss: 0.4726\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4043 - val_loss: 0.4722\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4027 - val_loss: 0.4708\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4009 - val_loss: 0.4704\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3998 - val_loss: 0.4714\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3985 - val_loss: 0.4704\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3970 - val_loss: 0.4718\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3958 - val_loss: 0.4712\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3944 - val_loss: 0.4701\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3936 - val_loss: 0.4718\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3924 - val_loss: 0.4717\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3912 - val_loss: 0.4704\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3903 - val_loss: 0.4735\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3892 - val_loss: 0.4738\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3884 - val_loss: 0.4729\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3873 - val_loss: 0.4716\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3864 - val_loss: 0.4731\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3855 - val_loss: 0.4720\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3846 - val_loss: 0.4721\n",
      "121/121 [==============================] - 0s 405us/step - loss: 0.4001\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  12.8s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7641 - val_loss: 28.0492\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 2.0504 - val_loss: 43.0472\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 1.6124 - val_loss: 37.0128\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 1.3603 - val_loss: 28.7538\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 1.1689 - val_loss: 20.6120\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 1.0259 - val_loss: 14.6245\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.9261 - val_loss: 10.5960\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.8594 - val_loss: 7.2861\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.8137 - val_loss: 5.1836\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.7810 - val_loss: 3.7344\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.7555 - val_loss: 2.7778\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7346 - val_loss: 1.9391\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.7165 - val_loss: 1.4673\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7002 - val_loss: 1.2321\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.6852 - val_loss: 0.9812\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.6712 - val_loss: 0.8534\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6583 - val_loss: 0.7166\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6459 - val_loss: 0.6424\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6340 - val_loss: 0.5949\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6226 - val_loss: 0.5764\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6116 - val_loss: 0.5809\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.6010 - val_loss: 0.6027\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5911 - val_loss: 0.6369\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5816 - val_loss: 0.6922\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5725 - val_loss: 0.7604\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5639 - val_loss: 0.8304\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5555 - val_loss: 0.8810\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5477 - val_loss: 0.9624\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5401 - val_loss: 0.9578\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5332 - val_loss: 1.0158\n",
      "121/121 [==============================] - 0s 397us/step - loss: 0.5490\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=   5.7s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9218 - val_loss: 4.3285\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.2869 - val_loss: 2.8653\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.9733 - val_loss: 1.8260\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.8526 - val_loss: 1.2974\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.7870 - val_loss: 0.9606\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7448 - val_loss: 0.7924\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.7141 - val_loss: 0.7158\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6902 - val_loss: 0.6616\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6699 - val_loss: 0.6363\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.6525 - val_loss: 0.6160\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6364 - val_loss: 0.5999\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6216 - val_loss: 0.5855\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6084 - val_loss: 0.5729\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5959 - val_loss: 0.5615\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5844 - val_loss: 0.5509\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5739 - val_loss: 0.5399\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5641 - val_loss: 0.5301\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5547 - val_loss: 0.5210\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5461 - val_loss: 0.5129\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5381 - val_loss: 0.5062\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5305 - val_loss: 0.4992\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5234 - val_loss: 0.4932\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.5169 - val_loss: 0.4875\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5106 - val_loss: 0.4857\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5050 - val_loss: 0.4783\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4996 - val_loss: 0.4746\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4945 - val_loss: 0.4700\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4897 - val_loss: 0.4676\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4851 - val_loss: 0.4687\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4810 - val_loss: 0.4618\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4771 - val_loss: 0.4607\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4732 - val_loss: 0.4630\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4697 - val_loss: 0.4583\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4663 - val_loss: 0.4643\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4633 - val_loss: 0.4591\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4603 - val_loss: 0.4562\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4576 - val_loss: 0.4539\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4550 - val_loss: 0.4547\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4525 - val_loss: 0.4534\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4502 - val_loss: 0.4523\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4478 - val_loss: 0.4613\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4460 - val_loss: 0.4593\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4438 - val_loss: 0.4497\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4420 - val_loss: 0.4544\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4402 - val_loss: 0.4533\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4383 - val_loss: 0.4497\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4367 - val_loss: 0.4470\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4352 - val_loss: 0.4470\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4336 - val_loss: 0.4532\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4322 - val_loss: 0.4549\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4307 - val_loss: 0.4534\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4295 - val_loss: 0.4594\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4281 - val_loss: 0.4535\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4269 - val_loss: 0.4484\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4255 - val_loss: 0.4489\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4244 - val_loss: 0.4465\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4232 - val_loss: 0.4489\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4221 - val_loss: 0.4514\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4209 - val_loss: 0.4499\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4198 - val_loss: 0.4441\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4188 - val_loss: 0.4476\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4178 - val_loss: 0.4501\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4170 - val_loss: 0.4432\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4160 - val_loss: 0.4385\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4151 - val_loss: 0.4381\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4141 - val_loss: 0.4440\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4134 - val_loss: 0.4354\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4126 - val_loss: 0.4381\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4117 - val_loss: 0.4341\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4109 - val_loss: 0.4395\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4101 - val_loss: 0.4340\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4094 - val_loss: 0.4407\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4084 - val_loss: 0.4309\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4079 - val_loss: 0.4328\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4071 - val_loss: 0.4349\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4063 - val_loss: 0.4346\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4056 - val_loss: 0.4339\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4049 - val_loss: 0.4333\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4042 - val_loss: 0.4281\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4035 - val_loss: 0.4354\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4029 - val_loss: 0.4322\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4022 - val_loss: 0.4299\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4015 - val_loss: 0.4292\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4008 - val_loss: 0.4342\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4000 - val_loss: 0.4219\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3995 - val_loss: 0.4290\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3987 - val_loss: 0.4329\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3982 - val_loss: 0.4306\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3975 - val_loss: 0.4337\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3968 - val_loss: 0.4251\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3962 - val_loss: 0.4237\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3957 - val_loss: 0.4191\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3950 - val_loss: 0.4238\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3943 - val_loss: 0.4203\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3937 - val_loss: 0.4209\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3932 - val_loss: 0.4215\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3926 - val_loss: 0.4251\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3920 - val_loss: 0.4161\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3915 - val_loss: 0.4199\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3910 - val_loss: 0.4242\n",
      "  1/121 [..............................] - ETA: 0s - loss: 0.4537WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "121/121 [==============================] - 0s 397us/step - loss: 0.3897\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  18.0s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 2.1013 - val_loss: 5.2312\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.8603 - val_loss: 26.5013\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7494 - val_loss: 40.6122\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.0991 - val_loss: 135.6917\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 1.3388 - val_loss: 237.1149\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 4.6734 - val_loss: 506.5569\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 5.0735 - val_loss: 1165.5585\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 19.0953 - val_loss: 2646.9749\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 28.1002 - val_loss: 5780.9756\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 97.2631 - val_loss: 13751.4180\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 159.4888 - val_loss: 31633.9141\n",
      "121/121 [==============================] - 0s 413us/step - loss: 81.5957\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   2.1s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 1.4769 - val_loss: 14.0701\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5769 - val_loss: 16.8410\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5493 - val_loss: 19.0635\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.5365 - val_loss: 19.7342\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.5272 - val_loss: 20.0593\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5202 - val_loss: 20.2376\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5153 - val_loss: 20.0296\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.5113 - val_loss: 20.3793\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.5078 - val_loss: 20.1103\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5063 - val_loss: 18.4892\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.5056 - val_loss: 19.4013\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.9640\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   2.2s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 2.0333 - val_loss: 13.7380\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.6240 - val_loss: 10.0594\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.7131 - val_loss: 41.2693\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 1.1121 - val_loss: 74.9048\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.9784 - val_loss: 205.5686\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 1.9726 - val_loss: 246.7374\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 2.5115 - val_loss: 388.8352\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 5.9673 - val_loss: 620.5344\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 6.9990 - val_loss: 919.7242\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 4.1843 - val_loss: 1082.5527\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 21.4948 - val_loss: 1471.0372\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 10.2613 - val_loss: 1957.3093\n",
      "121/121 [==============================] - 0s 355us/step - loss: 2.0491\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   2.4s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2457 - val_loss: 22.8634\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 1.1255 - val_loss: 36.5661\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.9314 - val_loss: 304.7440\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.8113 - val_loss: 71.4702\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.8365 - val_loss: 312.6021\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 2.8876 - val_loss: 0.4035\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4331 - val_loss: 0.3815\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3841 - val_loss: 0.3614\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3771 - val_loss: 0.3613\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3676 - val_loss: 0.3455\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3700 - val_loss: 0.3469\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3623 - val_loss: 0.3554\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3548 - val_loss: 0.3456\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3603 - val_loss: 0.3430\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3515 - val_loss: 0.3427\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3531 - val_loss: 0.3421\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3472 - val_loss: 0.3402\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3515 - val_loss: 0.3426\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3628 - val_loss: 0.3416\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3443 - val_loss: 0.3385\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3431 - val_loss: 0.3428\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3416 - val_loss: 0.3400\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3398 - val_loss: 0.3584\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3413 - val_loss: 0.3509\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3375 - val_loss: 0.3590\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3367 - val_loss: 0.3456\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3369 - val_loss: 0.3546\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3367 - val_loss: 0.3542\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3350 - val_loss: 0.3522\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3347 - val_loss: 0.3408\n",
      "121/121 [==============================] - 0s 397us/step - loss: 0.3580\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=   5.3s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8950 - val_loss: 3.0949\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.5223 - val_loss: 0.4712\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.4610 - val_loss: 0.4231\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.4356 - val_loss: 0.4021\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.4204 - val_loss: 0.4323\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4116 - val_loss: 0.6513\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4057 - val_loss: 0.8508\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4000 - val_loss: 1.0201\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3954 - val_loss: 1.1757\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3924 - val_loss: 0.8698\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3877 - val_loss: 0.9377\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3839 - val_loss: 1.0793\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3820 - val_loss: 1.1923\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3805 - val_loss: 1.1186\n",
      "121/121 [==============================] - 0s 405us/step - loss: 0.4037\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=   2.7s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9047 - val_loss: 1.2874\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.4870 - val_loss: 0.7809\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4394 - val_loss: 1.8555\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5436 - val_loss: 18.7095\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.5247 - val_loss: 78.6924\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.8047 - val_loss: 0.4362\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.4359 - val_loss: 0.3913\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4187 - val_loss: 0.4218\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4075 - val_loss: 0.4237\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4053 - val_loss: 0.3656\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3912 - val_loss: 0.4488\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3900 - val_loss: 0.3706\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3853 - val_loss: 0.3624\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3828 - val_loss: 0.4259\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3798 - val_loss: 0.3565\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3770 - val_loss: 0.4250\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3741 - val_loss: 0.3505\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3754 - val_loss: 0.3620\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3701 - val_loss: 0.4285\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3671 - val_loss: 0.3458\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3714 - val_loss: 0.3817\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3679 - val_loss: 0.3408\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3623 - val_loss: 0.3550\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3606 - val_loss: 0.4301\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3602 - val_loss: 0.3363\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3578 - val_loss: 0.3482\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3590 - val_loss: 0.4181\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3566 - val_loss: 0.3341\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3666 - val_loss: 0.3680\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3677 - val_loss: 0.3554\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3514 - val_loss: 0.3432\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3497 - val_loss: 0.4889\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3501 - val_loss: 0.3439\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3471 - val_loss: 0.4453\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3491 - val_loss: 0.3286\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3448 - val_loss: 0.3491\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3439 - val_loss: 0.3309\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3458 - val_loss: 0.3275\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3458 - val_loss: 0.3267\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3423 - val_loss: 0.4576\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3404 - val_loss: 0.3242\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3376 - val_loss: 0.6091\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3447 - val_loss: 0.3217\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3397 - val_loss: 0.3504\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3373 - val_loss: 0.3561\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3387 - val_loss: 0.4243\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3336 - val_loss: 0.3171\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3421 - val_loss: 0.3261\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3408 - val_loss: 0.4461\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3451 - val_loss: 0.3319\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3329 - val_loss: 0.5067\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3329 - val_loss: 0.3640\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3325 - val_loss: 0.3154\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3391 - val_loss: 0.3790\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3477 - val_loss: 0.3139\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3361 - val_loss: 0.3709\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3346 - val_loss: 0.3641\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3300 - val_loss: 0.3207\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3252 - val_loss: 0.4157\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3298 - val_loss: 0.3098\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3374 - val_loss: 0.3260\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3248 - val_loss: 0.3305\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3290 - val_loss: 0.3103\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3241 - val_loss: 0.4051\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3229 - val_loss: 0.3067\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3364 - val_loss: 0.3350\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3277 - val_loss: 0.3099\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3236 - val_loss: 0.3082\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3209 - val_loss: 0.3138\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3197 - val_loss: 0.4052\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3253 - val_loss: 0.3043\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3206 - val_loss: 0.3432\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3318 - val_loss: 0.3366\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3319 - val_loss: 0.3071\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3204 - val_loss: 0.4425\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3446 - val_loss: 0.4489\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3275 - val_loss: 0.3102\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3202 - val_loss: 0.3665\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3165 - val_loss: 0.3031\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3190 - val_loss: 0.3678\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3167 - val_loss: 0.3040\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3169 - val_loss: 0.3673\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3169 - val_loss: 0.3110\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3228 - val_loss: 0.4228\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3409 - val_loss: 0.3015\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3148 - val_loss: 0.3384\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3278 - val_loss: 0.3726\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3156 - val_loss: 0.3027\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3149 - val_loss: 0.4641\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3171 - val_loss: 0.3008\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3138 - val_loss: 0.3173\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3132 - val_loss: 0.3010\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3130 - val_loss: 0.3385\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3123 - val_loss: 0.4416\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3104 - val_loss: 0.3790\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3102 - val_loss: 0.5682\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3691 - val_loss: 0.3435\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3194 - val_loss: 0.3789\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3283 - val_loss: 0.4178\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3133 - val_loss: 0.3502\n",
      "  1/121 [..............................] - ETA: 0s - loss: 0.2950WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.3168\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  16.7s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 5.1910WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0446 - val_loss: 7.0502\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 2.3108 - val_loss: 7.2037\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 1.6259 - val_loss: 5.5884\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 1.3383 - val_loss: 3.7640\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 1.1769 - val_loss: 2.5552\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 1.0669 - val_loss: 2.0914\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.9873 - val_loss: 1.6989\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.9225 - val_loss: 1.4173\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.8695 - val_loss: 1.2066\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.8251 - val_loss: 1.0479\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7880 - val_loss: 0.9248\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.7571 - val_loss: 0.8264\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7314 - val_loss: 0.7581\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.7096 - val_loss: 0.7119\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6917 - val_loss: 0.6743\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6760 - val_loss: 0.6514\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6629 - val_loss: 0.6371\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.6514 - val_loss: 0.6283\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.6412 - val_loss: 0.6229\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.6322 - val_loss: 0.6221\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.6241 - val_loss: 0.6180\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6164 - val_loss: 0.6178\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.6092 - val_loss: 0.6150\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.6023 - val_loss: 0.6175\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5962 - val_loss: 0.6112\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5900 - val_loss: 0.6049\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.5840 - val_loss: 0.6013\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5783 - val_loss: 0.5932\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5726 - val_loss: 0.5873\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5671 - val_loss: 0.5833\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5619 - val_loss: 0.5789\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5567 - val_loss: 0.5713\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5516 - val_loss: 0.5664\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5467 - val_loss: 0.5614\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5418 - val_loss: 0.5537\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5371 - val_loss: 0.5536\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5326 - val_loss: 0.5419\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5281 - val_loss: 0.5418\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5237 - val_loss: 0.5343\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5195 - val_loss: 0.5282\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5153 - val_loss: 0.5252\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.515 - 0s 711us/step - loss: 0.5113 - val_loss: 0.5198\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5074 - val_loss: 0.5159\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5035 - val_loss: 0.5109\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4997 - val_loss: 0.5071\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4959 - val_loss: 0.5049\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4925 - val_loss: 0.4988\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4888 - val_loss: 0.4950\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4853 - val_loss: 0.4902\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4819 - val_loss: 0.4869\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4785 - val_loss: 0.4851\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4754 - val_loss: 0.4779\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4721 - val_loss: 0.4730\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4689 - val_loss: 0.4699\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4658 - val_loss: 0.4657\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4629 - val_loss: 0.4605\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4600 - val_loss: 0.4583\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4571 - val_loss: 0.4528\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4544 - val_loss: 0.4496\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4516 - val_loss: 0.4473\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4492 - val_loss: 0.4437\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4466 - val_loss: 0.4411\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4440 - val_loss: 0.4392\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4418 - val_loss: 0.4341\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4394 - val_loss: 0.4314\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4371 - val_loss: 0.4299\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4349 - val_loss: 0.4275\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4327 - val_loss: 0.4236\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4306 - val_loss: 0.4217\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4284 - val_loss: 0.4202\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4266 - val_loss: 0.4168\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4246 - val_loss: 0.4144\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4227 - val_loss: 0.4125\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4209 - val_loss: 0.4105\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4191 - val_loss: 0.4085\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4172 - val_loss: 0.4071\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4157 - val_loss: 0.4048\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4140 - val_loss: 0.4032\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.4125 - val_loss: 0.4017\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4108 - val_loss: 0.4002\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4094 - val_loss: 0.3986\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4079 - val_loss: 0.3973\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4065 - val_loss: 0.3959\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4051 - val_loss: 0.3949\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4037 - val_loss: 0.3931\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4022 - val_loss: 0.3926\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4012 - val_loss: 0.3914\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3999 - val_loss: 0.3897\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3987 - val_loss: 0.3888\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3975 - val_loss: 0.3880\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3964 - val_loss: 0.3866\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3953 - val_loss: 0.3861\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3943 - val_loss: 0.3848\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3932 - val_loss: 0.3839\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3922 - val_loss: 0.3836\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3911 - val_loss: 0.3840\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3904 - val_loss: 0.3822\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3894 - val_loss: 0.3806\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3883 - val_loss: 0.3822\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3876 - val_loss: 0.3796\n",
      "121/121 [==============================] - 0s 397us/step - loss: 0.3993\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  17.9s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0701 - val_loss: 2.9725\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 2.1451 - val_loss: 5.9015\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.2758 - val_loss: 10.8119\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 1.0903 - val_loss: 11.3108\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 1.0053 - val_loss: 9.9424\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.9444 - val_loss: 8.2069\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.8977 - val_loss: 6.6004\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.8609 - val_loss: 4.8507\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.8311 - val_loss: 3.5263\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.8061 - val_loss: 2.6353\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7845 - val_loss: 1.9734\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.7657 - val_loss: 1.4481\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.7491 - val_loss: 1.1077\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.7340 - val_loss: 0.8819\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.7201 - val_loss: 0.7221\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.7071 - val_loss: 0.6649\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6951 - val_loss: 0.6775\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.6837 - val_loss: 0.7491\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6729 - val_loss: 0.8815\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6627 - val_loss: 1.0685\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6528 - val_loss: 1.3065\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6436 - val_loss: 1.5428\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.6350 - val_loss: 1.8315\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6268 - val_loss: 2.1427\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6190 - val_loss: 2.5085\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6115 - val_loss: 2.8640\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.6770\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=   5.0s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4059 - val_loss: 3.5308\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 2.5613 - val_loss: 3.0045\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.4038 - val_loss: 2.5464\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.9815 - val_loss: 1.8717\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.8396 - val_loss: 1.3067\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7692 - val_loss: 0.9966\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.7270 - val_loss: 0.8331\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6993 - val_loss: 0.7309\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6799 - val_loss: 0.6922\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6652 - val_loss: 0.6623\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6528 - val_loss: 0.6391\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.6420 - val_loss: 0.6199\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6325 - val_loss: 0.6066\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6235 - val_loss: 0.5952\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.6150 - val_loss: 0.5855\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6070 - val_loss: 0.5761\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5992 - val_loss: 0.5671\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5915 - val_loss: 0.5590\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5841 - val_loss: 0.5515\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.5769 - val_loss: 0.5445\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5697 - val_loss: 0.5376\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5627 - val_loss: 0.5308\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5559 - val_loss: 0.5241\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5491 - val_loss: 0.5179\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5425 - val_loss: 0.5114\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.5360 - val_loss: 0.5049\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5296 - val_loss: 0.4989\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5234 - val_loss: 0.4930\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5173 - val_loss: 0.4880\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5115 - val_loss: 0.4819\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5058 - val_loss: 0.4768\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5003 - val_loss: 0.4726\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4951 - val_loss: 0.4680\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4901 - val_loss: 0.4647\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4854 - val_loss: 0.4611\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4809 - val_loss: 0.4579\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4767 - val_loss: 0.4552\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4727 - val_loss: 0.4523\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4688 - val_loss: 0.4503\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4651 - val_loss: 0.4481\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4616 - val_loss: 0.4479\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4584 - val_loss: 0.4464\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4552 - val_loss: 0.4442\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4524 - val_loss: 0.4442\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4496 - val_loss: 0.4436\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4469 - val_loss: 0.4428\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4444 - val_loss: 0.4428\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4420 - val_loss: 0.4431\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4397 - val_loss: 0.4436\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4375 - val_loss: 0.4441\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4353 - val_loss: 0.4441\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4335 - val_loss: 0.4456\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4315 - val_loss: 0.4460\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4297 - val_loss: 0.4467\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4278 - val_loss: 0.4484\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4262 - val_loss: 0.4490\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4247 - val_loss: 0.4493\n",
      "  1/121 [..............................] - ETA: 0s - loss: 0.5564WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "121/121 [==============================] - 0s 388us/step - loss: 0.4256\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  10.6s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 1.3002 - val_loss: 38.2652\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.9964 - val_loss: 0.6706\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5490 - val_loss: 0.5520\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4986 - val_loss: 0.5090\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4710 - val_loss: 0.4813\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.4526 - val_loss: 0.4761\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.4406 - val_loss: 0.4565\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4321 - val_loss: 0.4533\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4259 - val_loss: 0.4502\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4210 - val_loss: 0.4389\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.4166 - val_loss: 0.4360\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.4123 - val_loss: 0.4313\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4086 - val_loss: 0.4253\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.4064 - val_loss: 0.4228\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.4030 - val_loss: 0.4209\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.4011 - val_loss: 0.4192\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3986 - val_loss: 0.4156\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3966 - val_loss: 0.4137\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3949 - val_loss: 0.4128\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3936 - val_loss: 0.4104\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3928 - val_loss: 0.4101\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3908 - val_loss: 0.4070\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3898 - val_loss: 0.4080\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3892 - val_loss: 0.4037\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3878 - val_loss: 0.4030\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3869 - val_loss: 0.4000\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3864 - val_loss: 0.3972\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3851 - val_loss: 0.3974\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3838 - val_loss: 0.3943\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3834 - val_loss: 0.3948\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3818 - val_loss: 0.3922\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3817 - val_loss: 0.3917\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3803 - val_loss: 0.3905\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3798 - val_loss: 0.3893\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3790 - val_loss: 0.3876\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.3775 - val_loss: 0.3916\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3769 - val_loss: 0.3831\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3762 - val_loss: 0.3875\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3747 - val_loss: 0.3847\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3750 - val_loss: 0.3846\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3744 - val_loss: 0.3842\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3730 - val_loss: 0.3814\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3735 - val_loss: 0.3808\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3718 - val_loss: 0.3834\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3725 - val_loss: 0.3804\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3704 - val_loss: 0.3824\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3703 - val_loss: 0.3798\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3705 - val_loss: 0.3800\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3686 - val_loss: 0.3783\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3686 - val_loss: 0.3797\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3683 - val_loss: 0.3820\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3675 - val_loss: 0.3765\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3672 - val_loss: 0.3772\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3667 - val_loss: 0.3766\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3656 - val_loss: 0.3773\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3663 - val_loss: 0.3754\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3650 - val_loss: 0.3750\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3645 - val_loss: 0.3750\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3633 - val_loss: 0.3766\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3626 - val_loss: 0.3749\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3629 - val_loss: 0.3764\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3619 - val_loss: 0.3759\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3614 - val_loss: 0.3736\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3607 - val_loss: 0.3750\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3606 - val_loss: 0.3727\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3631 - val_loss: 0.3757\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3598 - val_loss: 0.3733\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3590 - val_loss: 0.3719\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3583 - val_loss: 0.3714\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3589 - val_loss: 0.3698\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3574 - val_loss: 0.3689\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3564 - val_loss: 0.3717\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3572 - val_loss: 0.3699\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3559 - val_loss: 0.3664\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3557 - val_loss: 0.3663\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3545 - val_loss: 0.3689\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3551 - val_loss: 0.3670\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3539 - val_loss: 0.3682\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3534 - val_loss: 0.3647\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3541 - val_loss: 0.3669\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3528 - val_loss: 0.3654\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3529 - val_loss: 0.3639\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3529 - val_loss: 0.3644\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3514 - val_loss: 0.3632\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3511 - val_loss: 0.3619\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3502 - val_loss: 0.3615\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3512 - val_loss: 0.3592\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3499 - val_loss: 0.3633\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3494 - val_loss: 0.3581\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3493 - val_loss: 0.3597\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3489 - val_loss: 0.3582\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3487 - val_loss: 0.3575\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3483 - val_loss: 0.3572\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3485 - val_loss: 0.3582\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3470 - val_loss: 0.3579\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3466 - val_loss: 0.3626\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3466 - val_loss: 0.3564\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3478 - val_loss: 0.3560\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3465 - val_loss: 0.3618\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3458 - val_loss: 0.3552\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.3645\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  16.5s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 1.2613 - val_loss: 0.6451\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.5785 - val_loss: 0.8942\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5115 - val_loss: 1.2421\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4809 - val_loss: 1.2691\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4597 - val_loss: 0.9915\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.4442 - val_loss: 0.6535\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4337 - val_loss: 0.5216\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4234 - val_loss: 0.4130\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4148 - val_loss: 0.3818\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4105 - val_loss: 0.4044\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4043 - val_loss: 0.4355\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3994 - val_loss: 0.4276\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3968 - val_loss: 0.4761\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3953 - val_loss: 0.5445\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3922 - val_loss: 0.5613\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3886 - val_loss: 0.6763\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3861 - val_loss: 0.6692\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3845 - val_loss: 0.7573\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3817 - val_loss: 0.6834\n",
      "121/121 [==============================] - 0s 388us/step - loss: 0.3963\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=   3.5s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 1.2040 - val_loss: 71.0120\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 1.0541 - val_loss: 42.2913\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.8661 - val_loss: 1.3112\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5287 - val_loss: 0.5968\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4829 - val_loss: 0.4855\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4636 - val_loss: 0.4448\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4508 - val_loss: 0.4217\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.4424 - val_loss: 0.4094\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4358 - val_loss: 0.4025\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4322 - val_loss: 0.3958\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.4267 - val_loss: 0.3918\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4230 - val_loss: 0.3892\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.4196 - val_loss: 0.3915\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4170 - val_loss: 0.3996\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4136 - val_loss: 0.4076\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.4108 - val_loss: 0.4218\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.4081 - val_loss: 0.4317\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.4068 - val_loss: 0.4354\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4048 - val_loss: 0.4429\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.4034 - val_loss: 0.4453\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.4021 - val_loss: 0.4460\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4001 - val_loss: 0.4415\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.3947\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=   4.0s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 7.7197 - val_loss: 43.0907\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 5.5377 - val_loss: 27.4372\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 4.0702 - val_loss: 17.4473\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 3.0672 - val_loss: 11.0914\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 2.3745 - val_loss: 7.0664\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.8936 - val_loss: 4.5088\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 1.5544 - val_loss: 2.9277\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.3139 - val_loss: 1.9631\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 1.1426 - val_loss: 1.3974\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 1.0191 - val_loss: 1.0599\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.9292 - val_loss: 0.8770\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.8636 - val_loss: 0.7898\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.8150 - val_loss: 0.7557\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.7787 - val_loss: 0.7567\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.7515 - val_loss: 0.7696\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.7308 - val_loss: 0.7947\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.7148 - val_loss: 0.8231\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.7022 - val_loss: 0.8540\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.6920 - val_loss: 0.8834\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.6838 - val_loss: 0.9089\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6772 - val_loss: 0.9189\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.6713 - val_loss: 0.9382\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.6664 - val_loss: 0.9536\n",
      "121/121 [==============================] - 0s 372us/step - loss: 0.6673\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=   3.9s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 7.6328 - val_loss: 25.5463\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 5.6957 - val_loss: 23.8232\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 4.3247 - val_loss: 22.6165\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 3.3439 - val_loss: 21.7670\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 2.6347 - val_loss: 21.1673\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 2.1177 - val_loss: 20.7451\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 1.7380 - val_loss: 20.4552\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 1.4573 - val_loss: 20.2628\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 1.2483 - val_loss: 20.1364\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 1.0921 - val_loss: 20.0567\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.9744 - val_loss: 20.0180\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.8854 - val_loss: 20.0099\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.8177 - val_loss: 20.0241\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.7659 - val_loss: 20.0459\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.7259 - val_loss: 20.0826\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.6949 - val_loss: 20.1289\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.6707 - val_loss: 20.1804\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.6517 - val_loss: 20.2376\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6366 - val_loss: 20.3030\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.6244 - val_loss: 20.3653\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.6146 - val_loss: 20.4378\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6066 - val_loss: 20.5061\n",
      "121/121 [==============================] - 0s 355us/step - loss: 1.0973\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=   3.8s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 6.1564 - val_loss: 7.6683\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 4.4886 - val_loss: 4.9412\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 3.3699 - val_loss: 3.3299\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 2.6029 - val_loss: 2.3535\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 2.0673 - val_loss: 1.7864\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 1.6878 - val_loss: 1.4390\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 1.4151 - val_loss: 1.2303\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 1.2174 - val_loss: 1.1115\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 1.0734 - val_loss: 1.0396\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.9677 - val_loss: 0.9896\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.8894 - val_loss: 0.9739\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.8312 - val_loss: 0.9570\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.7875 - val_loss: 0.9426\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.7545 - val_loss: 0.9414\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.7294 - val_loss: 0.9351\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.7100 - val_loss: 0.9457\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6951 - val_loss: 0.9437\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.6832 - val_loss: 0.9404\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.6737 - val_loss: 0.9554\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6661 - val_loss: 0.9559\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.6596 - val_loss: 0.9558\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.6543 - val_loss: 0.9576\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6496 - val_loss: 0.9476\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6455 - val_loss: 0.9447\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6419 - val_loss: 0.9405\n",
      "121/121 [==============================] - 0s 339us/step - loss: 0.6455\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=   4.4s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3724 - val_loss: 19.2760\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.8289 - val_loss: 4.6055\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5845 - val_loss: 0.7004\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4829 - val_loss: 0.5034\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4459 - val_loss: 0.4495\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4238 - val_loss: 0.4262\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4095 - val_loss: 0.4112\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3994 - val_loss: 0.4155\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3919 - val_loss: 0.4120\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3855 - val_loss: 0.4010\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3805 - val_loss: 0.4074\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3762 - val_loss: 0.3889\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3711 - val_loss: 0.3859\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3705 - val_loss: 0.4045\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3652 - val_loss: 0.3846\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3648 - val_loss: 0.3960\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3611 - val_loss: 0.4089\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3584 - val_loss: 0.3869\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3556 - val_loss: 0.3831\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3541 - val_loss: 0.3809\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3534 - val_loss: 0.3896\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3503 - val_loss: 0.3832\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3493 - val_loss: 0.4091\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3492 - val_loss: 0.3679\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3456 - val_loss: 0.3796\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3455 - val_loss: 0.3614\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3445 - val_loss: 0.3639\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3421 - val_loss: 0.3727\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3407 - val_loss: 0.3736\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3396 - val_loss: 0.3584\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3373 - val_loss: 0.3479\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3377 - val_loss: 0.3440\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3358 - val_loss: 0.3357\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3354 - val_loss: 0.3444\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3337 - val_loss: 0.3285\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3322 - val_loss: 0.3482\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3320 - val_loss: 0.3243\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3304 - val_loss: 0.3493\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3293 - val_loss: 0.3234\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3301 - val_loss: 0.3429\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3288 - val_loss: 0.3282\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3267 - val_loss: 0.3236\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3278 - val_loss: 0.3246\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3251 - val_loss: 0.3410\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3269 - val_loss: 0.3236\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3235 - val_loss: 0.3294\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3236 - val_loss: 0.3368\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3236 - val_loss: 0.3343\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3212 - val_loss: 0.3173\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3211 - val_loss: 0.3414\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3200 - val_loss: 0.3386\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3188 - val_loss: 0.3331\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3190 - val_loss: 0.3262\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3178 - val_loss: 0.3184\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3160 - val_loss: 0.3237\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3161 - val_loss: 0.3197\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3148 - val_loss: 0.3317\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3140 - val_loss: 0.3240\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3135 - val_loss: 0.3102\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3126 - val_loss: 0.3207\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3123 - val_loss: 0.3261\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3112 - val_loss: 0.3356\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3112 - val_loss: 0.3113\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3101 - val_loss: 0.3329\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3093 - val_loss: 0.3244\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3094 - val_loss: 0.3059\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3086 - val_loss: 0.3231\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3078 - val_loss: 0.3250\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3069 - val_loss: 0.3158\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3069 - val_loss: 0.3114\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3058 - val_loss: 0.3195\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3052 - val_loss: 0.3120\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3045 - val_loss: 0.3047\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3044 - val_loss: 0.3233\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3036 - val_loss: 0.3107\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3024 - val_loss: 0.3151\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3030 - val_loss: 0.3017\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3019 - val_loss: 0.3221\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3026 - val_loss: 0.3046\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3004 - val_loss: 0.3125\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2997 - val_loss: 0.3010\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2994 - val_loss: 0.3379\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2990 - val_loss: 0.2980\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.2983 - val_loss: 0.3537\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2980 - val_loss: 0.2976\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2968 - val_loss: 0.3239\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2969 - val_loss: 0.3021\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2967 - val_loss: 0.3215\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2959 - val_loss: 0.2974\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2948 - val_loss: 0.3185\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.2950 - val_loss: 0.2950\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2942 - val_loss: 0.3171\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2940 - val_loss: 0.2982\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.2940 - val_loss: 0.2946\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2927 - val_loss: 0.3052\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2914 - val_loss: 0.2966\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2918 - val_loss: 0.3231\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2914 - val_loss: 0.2903\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2908 - val_loss: 0.3540\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.2907 - val_loss: 0.2961\n",
      "121/121 [==============================] - 0s 397us/step - loss: 0.3199\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=  17.4s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2201 - val_loss: 0.8642\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.6048 - val_loss: 0.7994\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5340 - val_loss: 1.0803\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4912 - val_loss: 1.1494\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.4606 - val_loss: 0.9498\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4414 - val_loss: 0.6208\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.4270 - val_loss: 0.4657\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.4148 - val_loss: 0.3888\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.4041 - val_loss: 0.4084\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3972 - val_loss: 0.4312\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3899 - val_loss: 0.5341\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3833 - val_loss: 0.6081\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3796 - val_loss: 0.7209\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3760 - val_loss: 0.8821\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3715 - val_loss: 0.9049\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3680 - val_loss: 0.9792\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3647 - val_loss: 0.9532\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3628 - val_loss: 1.0397\n",
      "121/121 [==============================] - 0s 388us/step - loss: 0.3909\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=   3.5s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "224/242 [==========================>...] - ETA: 0s - loss: 1.1576WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1300 - val_loss: 2.2824\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.6910 - val_loss: 2.5063\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5904 - val_loss: 1.3345\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.5360 - val_loss: 1.8303\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.4879 - val_loss: 1.1690\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4597 - val_loss: 1.0937\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4408 - val_loss: 0.5393\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4235 - val_loss: 0.5528\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4125 - val_loss: 0.4217\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4079 - val_loss: 0.3978\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3958 - val_loss: 0.7642\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3934 - val_loss: 0.3953\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3853 - val_loss: 0.3690\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3805 - val_loss: 0.6782\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3793 - val_loss: 0.5137\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3750 - val_loss: 1.5716\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3863 - val_loss: 1.5438\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3814 - val_loss: 2.5256\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3989 - val_loss: 1.2077\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3812 - val_loss: 0.8839\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3663 - val_loss: 0.3408\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3590 - val_loss: 0.3928\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3572 - val_loss: 0.3411\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3572 - val_loss: 0.4823\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3555 - val_loss: 0.3589\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3527 - val_loss: 0.3810\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3504 - val_loss: 0.4593\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3500 - val_loss: 0.3360\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3499 - val_loss: 0.4983\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3480 - val_loss: 0.3747\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3470 - val_loss: 0.4128\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3445 - val_loss: 0.5465\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3461 - val_loss: 0.3826\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3426 - val_loss: 0.5040\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3434 - val_loss: 0.3439\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3398 - val_loss: 0.4821\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3404 - val_loss: 0.3595\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3414 - val_loss: 0.6279\n",
      "  1/121 [..............................] - ETA: 0s - loss: 0.3054WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "121/121 [==============================] - 0s 388us/step - loss: 0.3388\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001C2FF207CD0>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-5ae18773bdf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[0m\u001b[0;32m     12\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    736\u001b[0m                 **self.best_params_))\n\u001b[0;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     81\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001C2FF207CD0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.34988949696222943"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-3b40e07bb81d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnd_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'scorer_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-cbbd136c11f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnd_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             raise ValueError(\"No score function explicitly defined, \"\n\u001b[0;32m    445\u001b[0m                              \u001b[1;34m\"and the estimator doesn't provide one %s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'scorer_'"
     ]
    }
   ],
   "source": [
    "rnd_search_cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-f89828bd4b63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/162 [..............................] - ETA: 0s - loss: 1.5922WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "162/162 [==============================] - 0s 401us/step - loss: 584431.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "584431.75"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos guardar nuestro modelo, evaluarlo en el conjunto de prueba y, si estamos satisfechos con su rendimiento, desplegarlo en producción. Usar búsqueda aleatoria no es demasiado duro, y funciona bien para la mayoría de problemas simples. Sin embargo, cuando el entrenamiento es lento (por ejemplo, para problemas más complejos con datasets grandes) este enfoque solo explorará una pequeña porción del espacio de de hiperparámetros. Podemos aliviar parcialmente este problema ayudando al proceso de búsqueda manualmente: primero ejecutamos una búsqueda aleatoria rápida usando amplis rangos de valores de hiperparámetros, después ejecutamos otra búsqueda usando rangos más pequeños de valores centrados en los mejores encontrados durante la primera ejecución, y así sucesivamente. Es de esperar que este enfoque se centre en un buen conjunto de hiperparámetros. Sin embargo, consume mucho tiempo y probablemente no sea el mejor uso de nuestro tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afortunadamente, existen muchas técnicas para explorar un espacio de búsqueda mucho más eficientemente que de forma aleatoria. La idea core es simple: cuando una región del espacio resulta ser buena hay que explorarla más. Tales técnicas se encargan de \"focalizar\" el proceso por nosotros y conducen a soluciones mucho mejores en menos tiempo. He aquí algunas librerías Python que podemos usar para optimizar hiperparámetros:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Hyperopt](https://github.com/hyperopt/hyperopt)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una popular librería para optimización sobre todo tipo de espacios de búsqueda complejos (incluyendo valores reales, tales como tasas de aprendizaje, y valores discretos, como el número de capas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Hyperas](https://github.com/maxpumperla/hyperas), [kopt](https://github.com/Avsecz/kopt), [Talos](https://github.com/autonomio/talos)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Útiles librerías para la optimización de parámetros de modelos Keras (los primeros dos están basados en Hyperopt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Keras Tuner](https://homl.info/kerastuner)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trata de una librería de optimización de hiperparámetros de Google para modelos de Keras muy fácil de usar, con un servicio hosteado para visualización y análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Scikit-Optimize (`skopt`)](https://scikit-optimize.github.io/stable/)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una librería de optimización de propósito general. La clase `BayesSearchCV` ejecuta optimización bayesiana utilizando un interfaz similar a `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Spearmint](https://github.com/JasperSnoek/spearmint)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una librería de optimización bayesiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Hyperband](https://github.com/zygmuntz/hyperband)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una librería de ajuste de hiperparámetros rápida basada en el reciente [artículo de Hyperband](https://homl.info/hyperband) de Lisha Li."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Sklearn-Deap](https://github.com/rsteca/sklearn-deap)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una librería de optimización de hiperparámetros basada en algoritmos evolutivos, con un interfaz tipo `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, muchas compañías ofrecen servicios de optimización de hiperparámetros. En el capítulo 19 veremos el [servicio de ajuste de hiperparámetros](https://homl.info/googletuning) de la Plataforma AI de Google Cloud. Otras opciones incluyen servicio de [Arimo](https://arimo.com/), [SigOpt](https://sigopt.com/) y el CallDesk de [Oscar](http://oscar.calldesk.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ajuste de hiperparámetros es todavía un área de investigación activa y los algoritmos evolutivos están regresando. Por ejemplo, consulta el excelente [artículo de 2017](https://homl.info/pbt) de DeepMind, donde los autores optimizan conjuntamente una población de modelos y sus hiperparámetros. Google también ha usado un enfoque evolutivo, no solo para buscar hiperparámetros sino para buscar la mejor arquitectura de red neuronal para el problema; su suite AutoML también está disponible como [servicio en la nube](https://cloud.google.com/automl/). ¿Quizás los días de construir manualmente redes neuronales esté llegando a su fin? Consulta el [post](https://homl.info/automlpost) de Google sobre este tema. De hecho, los algoritmos evolutivos se han usado con éxito para entrenar redes neuronales individuales, ¡reemplazando al ubicuo Descenso de Gradiente! Como ejemplo, consulta el [artículo de 2017](https://homl.info/neuroevol) de Uber donde los autores presentan su técnica de *Neuroevolución Profunda* (*Deep Neuroevolution*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero a pesar de todos estos excitantes progresos y todas estas herramientas y técnicas, todavía ayuda tener una idea de qué valores son razonables para cada hiperparámetro, para poder construir un prototipo rápido y restringir el espacio de búsqueda. Las siguientes secciones proporcionan pautas para elegir el número de capas ocultas y neuronas de un MLP y para seleccionar algunos buenos valores para los principales hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número de capas ocultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la mayoría de los problemas, podemos comenzar con una única capa oculta y obtendremos resultados razonbles. Un MLP con una única capa oculta puede modelar teóricamente incluso las funciones más complejas, siempre que tenga suficiente neuronas. Pero para problemas complejos, las redes profundas tienen una *eficiencia de parámetros* mucho mayor que las superficiales: pueden modelar funciones complejas usando exponencialmente menos neuronas que las redes superficiales, permitiéndoles alcanzar mucho mejor rendimiento con la misma cantidad de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comporender por qué, supongamos que se nos pide dibujar un bosque usando algún software de dibujo, pero tenemos prohibido copiar y pegar nada. Nos llevaría una enorme cantidad de tiempo: tendríamos que dibujar individualmente cada árbol, rama por rama, hoja por hoja. Si pudiéramos dibujar una hoja, copiarla y pegarla para dibujar una rama, luego copiar y pegar esa rama para crear un árbol y, finalmente, copiar y pegar ese árbol para hacer el bosque, terminaríamos en poco tiempo. Los datos del mundo real a menudo se estructuran de esta forma jerárquica y las redes neuronales profundas se aprovechan automáticamente de este hecho: las capas ocultas inferiores modelan estructuras de bajo nivel (por ejemplo, segmentos de lineas de varios tamaños y orientaciones), las capas ocultas intermedias combinan estas estructuras de bajo nivel para modelar estructuras de nivel intermedio (por ejemplo, cuadrados, círculos) y las capas ocultas superiores y la capa de salida combinan esas estructuras intermedias para modelar estructuras de alto nivel (por ejemplo, caras)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta arquitectura jerárquica no solo ayuda a las DNNs a converger más rápidamente a una buena solución, sino que mejoran su habilidad para generalizar a nuevos datasets. Por ejemplo, si hemos entrenado un modelo para reconocer caras en fotografías y ahora queremos entrenar una nueva red neuronal para que reconozca peinados, podemos iniciar el entrenamiento reutilizando las capas inferiores del primer modelo. En lugar de inicializar aleatoriamente los pesos y los sesgos de las primeras capas de la nueva red neuronal, podemos inicializarlas con los valores de los pesos y los sesgos de las capas inferiores del primer modelo. De esta forma la red no tendrá que aprender desde cero todas las estructuras de bajo nivel que aparecen en la mayoría de las fotografías; solo tendrá que aprender las estructuras de alto nivel (es decir, los peinados). Esto se denomina *aprendizaje por transferencia*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, para la mayoría de los problemas podemos empezar solo con una o dos capas ocultas y la red neuronal trabajará bien. Por ejemplo, podemos alcanzar fácilmente alrededor del 97% de precisión en el dataset MNIST usando solo una capa oculta con algunos cientos de neuronas y alrededor del 98% de precisión usando dos capas ocultas con el mismo número total de neuronas, en aproximadamente la misma cantidad de tiempo de entrenamiento. Para problemas más complejos, podemos aumentar el número de capas ocultas hasta que empecemos a sobreajustar el conjunto de entrenamiento. Para tareas muy complejas, como clasificación de imágenes grandes o reconocimiento de habla, normalmente se requieren redes con docenas de capas (o incluso cientos, pero no totalmente conectadas, como veremos en el capítulo 14) y se necesitan cantidades enormes de datos de entrenamiento. Raramente tendremos que entrenar tales redes desde cero: es mucho más habitual reutilizar partes de redes preentrenadas que ejecutan tareas similares. Entonces el entrenamiento será más rápido y requerirá menos datos (como veremos en el capítulo 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número de neuronas por capa oculta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de neuronas de las capas de entrada y salida está determinado por el tipo de entrada y salida que requiera nuestra tarea. Por ejemplo, la tarea MNIST requiere 28 x 28 = 784 neuronas de entrada y 10 neuronas de salida.\n",
    "\n",
    "En cuanto a las capas ocultas, suele ser común dimensionarlas para formar una pirámide, con cada vez menos neuronas en cada capa. La razón es que muchas de las características de bajo nivel pueden fusionarse en muchas menos características de alto nivel. Una red neuronal normal para MNIST puede tener 3 capas ocultas, la primera con 300 neuronas, la segunda con 200 y la tercera con 100. Sin embargo, esta práctica ha sido abandonada en gran parte porque parece que usar el mismo número de neuronas en todas las capas ocultas funciona igual de bien en la mayoría de los casos, o incluso mejor; además, solo hay un único hiperparámetro que ajustar, en lugar de uno por capa. Dicho esto, dependiendo del dataset, algunas veces puede ayudar hacer la primera capa oculta más grande que las otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que el número de capas, podemos intentar aumentar el número de neuronas de forma gradual hasta que la red empiece a sobreajustarse. Pero en la práctica, es a menudo más simple y más eficiente elegir un modelo con más capas y neuronas de las que realmente se necesita y posteriormente usar la detención temprana y otras técnicas de regularización para prevenir el sobreajuste. Vincent Vanhoucke, un científico de Google, lo ha denominado el enfoque de los \"pantalones elásticos\": en lugar de gastar tiempo tratando que los pantalones se ajusten perfectamente a nuestra talla, usemos grandes pantalones elásticos que se ajuster perfectamente a nuestra talla. Con este enfoque, evitamos las capas de cuello de botella que podrían arruinar nuestro modelo. En la otra cara, si una capa tiene demasiadas pocas neuronas, no tendrá suficiente poder representacional para preservar toda la información útil de las entradas (por ejemplo, una capa con dos neuronas solo puede devolver datos 2D, por lo que si procesa datos 3D se perderá alguna información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "En general, obtendremos más beneficios invirtiendo en incrementar el número de capas que en el número de neuronas por capa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasa de aprendizaje, tamaño de lote y otros hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de capas ocultas y neuronas no son los únicos hiperparámetros que podemos ajustar en un MLP. Aquí presentamos algunos de los más importantes, así como algunas pautas de cómo configurarlos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Tasa de aprendizaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tasa de aprendizaje es posiblemente el hiperparámetro más importante. En general, la tasa de aprendizaje óptima es aproximadamente la mitad de la tasa de aprendizaje máxima (es decir, la tasa de aprendizaje por encima de la cual el algoritmo de entrenamiento diverge, como vimos en el capítulo 4). Una forma de encontrar una buena tasa de aprendizaje es entrenar el modelo durante unos cientos de iteraciones, empezando con una tase de aprendizaje muy baja (por ejemplo, 10$^{-5}$) e incrementarla gradualmente hasta un valor muy alto (por ejemplo, 10). Esto se hace multiplicando la tasa de aprendizaje por una factor constante en cada iteración (por ejemplo, por exp(log(10$^6$)/500), para pasar de 10$^{-5}$ a 10 en 500 iteraciones. Si dibujamos la pérdida como una función de la tasa de aprendizaje (usando una escala logarítmica para la tasa de aprendizaje), veríamos que disminuye al principio, pero después de un tiempo, la tasa de aprendizaje será demasiado grande, por lo que la pérdida se disparará nuevamente: la tasa de aprendizaje óptima será un poco más baja que el punto en el cual la pérdida empieza a aumentar (normalmente unas 10 veces más baja que el punto de inflexión). Entonces podemos reiniciar nuestro modelo y entrenarlo normalmente usando esta tasa de aprendizaje buena. Veremos más técnicas de tasas de aprendizaje en el capítulo 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Optimizador**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es muy importante elegir un optimizador mejor que el antiguo descenso de gradiente por mini-lotes (y ajuste de sus hiperparámetros). Veremos varios optimizadores avanzados en el capítulo 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Tamaño del lote**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tamaño del lote puede tener un impacto significativo en la ejecución de nuestro modelo y en el tiempo de entrenamiento. El principal beneficio de usar tamaños de lote grandes es que los aceleradores de hardware como las GPUs pueden procesarlo eficientemente (ver capítulo 19), por lo que el algoritmo de entrenamiento verá más intancias por segundo. Por lo tanto, muchos investigadores y practicamente recomiendan usar el tamaño de lote más grande que pueda caber en la RAM de la GPU. Sin embargo, hay un problema: en la práctica, tamaños de lote grandes conducen a inestabilidades en el entrenamiento, especialmente al principio del entrenamiento y el modelo resultante puede que no generalice tan bien como un modelo entrenado con un tamaño de lote pequeño. En abril de 2018, Yann LeCun incluso tuiteó \"Los amigos no permiten que los amigos usen mini-lotes de más de 32\", citando un [artículo de 2018](https://homl.info/smallbatch) de Dominic Masters y Carlo Luschi que concluían usar lotes pequeños (de 2 a 32) era preferible porque los lotes pequeños conducen a mejores modelos en menos tiempo de entrenamiento. Otros artículos apuntan en la dirección contraria, sin embargo; en 2017, los artículos de [Elad Hoffer](https://homl.info/largebatch) y [Priya Goyal](https://homl.info/largebatch2) mostraban que era posible usar tamaños de lotes muy grandes (hasta 8.192) usando varias técnicas como calentar la tasa de aprendizaje(es decir, comenzamos entrenando con una tasa de aprendizaje pequeña y después la aumentamos, como veremos en el capítulo 11). Esto conduce a un tiempo de entrenamiento muy pequeño, sin ningún problema de generalización. Por tanto, una estrategia es intentar utilizar un tamaño de lote grande, usando calentamiento de la tasa de aprendizaje y si el entrenamiento es inestable o la ejecución final es decepcionante, entonces intentaremos usar en su lugar un tamaño de lote pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Función de activación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente es este capítulo, discutimos cómo elegir la función de activación: en general, la función de activación ReLU será un buen punto de partida para todas las capas ocultas. Para la capa de salida depende de nuestra tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Número de iteraciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la mayoría de los casos el número de iteraciones de entrenamiento no necesita ser ajustado: en su lugar usamos la detención temprana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "La tasa de aprendizaje óptima depende de otros hiperparámetros, especialmente del tamaño de lote, por lo que si modificamos cualquier hiperparámetro, también tenemos que asegurarnos de actualizar la tasa de aprendizaje.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitddc70b56f9d74bcebb5cbcc975b589c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
