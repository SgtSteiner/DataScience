{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCCION A LAS REDES NEURONALES ARTIFICIALES CON KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pájaros nos inspiraron para volar, las plantas de bardana inspiraron el velcro e innumerables inventos más se inspiraron en la naturaleza. Parece lógico, entonces, mirar la arquitectura del cerebro en busca de inspiración sobre cómo construir una máquina inteligente. Esta es la idea clave que sirvió de chispa para las *redes neuronales artificiales - RNAs (artificial neuronal networks - ANN)*. Sin embargo, aunque los aviones se inspiraron en los pájaros, no tienen que batir sus alas. De forma similiar, las RNAs se han vuelto gradualmente bastante diferentes de sus primos biológicos. Algunos investigadores incluso argumentan que deberíamos abandonar por completo la analogía biológica (por ejemplo, diciendo \"unidades\" en lugar de \"neuronas\"), para que no restrinjamos nuestra creatividad a sistemas biológicamente plausibles (*podemos obtener lo mejor de ambos mundos estando abiertos a inspiraciones biológicas sin temor a crear modelos biológicamente irreales, siempre que funcionen bien*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las RNAs son el corazón de Deep Learning. Son versátiles, poderosas y escalables, haciéndolas ideales para abordar tareas grandes y altamente complejas de machine learning, tales como la clasificación de billones de imágenes (por ejemplo, Google Images), potenciar los servicios de reconocimiento del habla (por ejemplo, Siri de Apple), recomendar los mejores videos para ver a cientos de millones de usuarios cada dia (por ejemplo, YouTube) o aprendiendo a vencer al campeón del mundo del juego de *Go* jugando millones de partidas contra sí misma (Alpha-Zero de DeepMind)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la primera parte de este capítulo, nos introduciremos en las redes neuronales artificiales, empezando por un rapido recorrido por las primera arquitecturas de RNAs, hasta llegar a los *Perceptrones Multi-Capa PMCs* (*Multi-Layer Perceptrons, MLPs*), que se utilizan mucho en la actualidad (en otros capítulos exploraremos otras arquitecturas). En la segunda parte, echaremos un vistazo a cómo implementar RNAs usando la popular API Keras. Se trata de una API de alto nivel maravillosamente diseñada para la construcción, entrenamiento, evaluación y ejecución de redes neuronales. Pero no nos dejemos engañar por su simplicidad: es lo suficientemente expresiva y flexible para construir una amplia variedad de arquitecturas de redes neuronales. De hecho, probablemente será suficiente para la mayoría de nuestros casos. Además, si alguna vez necesitamos flexibilidad extra, siempre podremos escribir componentes de Keras personalizados usando su API de bajo nivel, como veremos más adelante.\n",
    "\n",
    "Pero primero, retrocedamos en el tiempo para ver cómo surgieron las redes neuronales artificiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De las neuronas biológicas a las artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorprendentemente, las RNAs existen desde hace bastante poco: fueron introducidas por primera vez en 1943 por el neurofisiólogo Warren McCulloch y el matemático Walter Pitts. En su [Artículo](https://homl.info/43) “A Logical Calculus of Ideas Immanent in Nervous Activity”, McCulloh y Pitts presentaron un modelo computacional simplificado de cómo las neuronas biológicas podrían trabajar juntas en el cerebro de los animales para ejecutar cálculos complejos usando *lógica proposicional*. Esta fue la primera arquitectura de red neuronal artifical. Desde entonces, se han inventado otras muchas arquitecturas, como veremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los primeros éxitos de las RNAs hasta la década de 1960 llevaron a la creencia generalizada de que pronto estaríamos conversando con máquinas verdaderamente inteligentes. Cuando quedó claro que esa promesa no sería cumplida (al menos durante bastante tiempo), los fondos y la inversión volaron hacia otra parte y las RNAs entraron en un largo invierno. A principio de la década de 1980 se produjo un resurgimiento del interés en el *conexionismo* (el estudio de las redes neuronales), a medida que se inventaron nuevas arquitecturas y se desarrollaron mejores técnicas de entrenamiento. Pero el progreso fue lento y en la década de 1990 se inventaron otras poderosas técnicas de Machine Learning, tales como las Máquinas de Soporte Vectorial. Estas técnicas parecían ofrecer mejores resultados y bases teóricas más solidas que las RNAs, así que una vez más el estudio de las redes neuronales entró en hibernación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, ahora estamos presenciando otra ola de interés en las RNAs. ¿Esta ola se extinguirá como las anteriores? Buieno, exsiten buenas razones para creer que esta ola es diferente y que tendrá un impacto mucho más profundo en nuestras vidas:\n",
    "\n",
    "+ Ahora existe una enorme cantidad de datos disponibles para entrenar a las redes neuronales y las RNAs superan frecuentemente a otras técnicas de ML en problemas muy grandes y complejos.\n",
    "\n",
    "+ El tremendo incremento en poder computacional desde la década de 1990 posibilita ahora entrenar grandes redes neuronales en una cantidad de tiempo razonable. Esto es en parte debido a la Ley de Moore, pero también gracias a la industria del juego, que ha producido por millones poderosas tarjetas GPU.\n",
    "\n",
    "+ Los algoritmos de entrenamiento han sido mejorados. Para ser justos, solo son ligeramente diferentes de los usados en la década de 1990, pero estos pequeños ajustes han tenido un impacto enorme.\n",
    "\n",
    "+ Algunas limitaciones teóricas de las RNAs han resultado ser benignas en la práctica. Por ejemplo, mucha gente pensó que los algoritmos de entrenamiento estaban condenados porque era probable que se atascaran en óptimos locales, pero resulta que esto es bastante raro en la práctica (o cuando es el caso, generalmente están bastante cerca del óptimo global).\n",
    "\n",
    "+ Las RNAs parecen haber entrado en un círculo virtuoso de financiación y progreso. Asombrosos productos basados en RNAs aparecen regularmente en las portadas de las noticias, lo que atrae cada vez más la atención y financiación hacia ellos, resultando en más y más progreso e incluso en más productos asombrosos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronas biológicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de que discutamos las neuronas artificiales, echemos un rápido vistazo a las neuronas biológicas (representada en la siguiente figura). Es una célula de aspecto inusual que se encuentra en la corteza cerebral animal (por ejemplo, nuestro cerebro), compuesta por un *cuerpo celular*, conteniendo el núcleo y la mayoría de los componentes complejos de la célula, y muchas extensiones ramificadas denominadas *dendritas*, más una larga extensión llamado *axón*. La longitud del axón puede ser solo un poco más largo que el cuerpo de la célula o hasta decenas de miles de veces más largo. Cerca de su extremidad, el axón se divide en muchas ramas llamadas *telodendritas* y en la punta de esas ramificaciones hay minúsculas estructuras llamadas *terminales sinápticas* (o simplemente *sinapsis*), que están conectadas a las dendritas (o directamente al cuerpo de la célula) de otras neuronas. Las neuronas biológicas reciben impulsos eléctricos cortos llamados *señales* de otras neuronas a través de estas sinapsis. Cuando una neurona recibe un número suficiente de señales de otras neuronas en unos pocos milisegundos, dispara sus propias señales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biological_neuron](images/ch10/biological_neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, las neuronas biológicas individuales parecen comportarse de una manera bastante simple, pero están organizadas en un vasta red de billones de neuronas, cada neurona conectada normalmente a miles de otras neuronas. Se pueden realizar cálculos altamente complejos mediante una red de neuronas bastante simples, al igual que un hormiguero puede emerger de los esfuerzos combinados de simples hormigas. La arquitectura de las redes neuronales biológicas (RNB) es todavía objeto de investigación activa, pero se han mapeado algunas partes del cerebro y parece que a menudo las neuronas se organizan en capas consecutivas, como se muestra en la siguiente figura, que representa las múltiples capas de una RNB del córtex humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multiple_layers](images/ch10/multiple_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculos lógicos con neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warren McCulloch y Walter Pitts propusieron un modelo muy simple de neurona biológica, que posteriormente se conoció como *neurona artificial*: tenía una o mas entradas binarias (on/off) y una salida binaria. La neurona artifical simplemente activa su salida cuando cierto número de sus entradas están activas. MacCulloch y Pitts mostraron que incluso con este modelo simplificado es posible construir una red de neuronas artificiales que pueden calcular cualquier proposición lógica que se quiera. Por ejemplo, construyamos algunas RNAs para ejecutar varios cálculos lógicos (ver siguiente figura), asumiendo que una neurona se activa cuando al menos dos de sus entradas están activas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rna_performing](images/ch10/rna_performing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ La primera red de la izquierda simplemente es la función identidad: si una neurona A está activada, la neurona C también se activa (dado que recibe dos señales de entrada de la neurona A), pero si la neurona A está apagada, la neurona C también está apagada.\n",
    "\n",
    "+ La segunda red ejecuta un AND lógico: la neurona C está activada solo cuando las neuronas A y B están activadas (una única señal de entrada no es suficiente para activar la neurona C).\n",
    "\n",
    "+ La tercera red ejecuta un OR lógico: la neurona C se activa si cualquiera de las neuronas A o B está activa (o ambas).\n",
    "\n",
    "+ Finalmente, si suponemos que una conexión de entrada puede inhibir la actividad de una neurona (como es el caso de las neuronas biológicas), entonces la cuarta red calcula una proposición lógica ligeramente más compleja: la neurona C es activada solo si la neurona A está activada y si la neurona B está apagada. Si la neurona A está activada todo el tiempo, entonces tendremos un NOT lógico: la neurona C está activada cuando la neurona B está apagada, y viceversa.\n",
    "\n",
    "Podemos imaginar fácilmente cómo podemos combinar estas redes para calcular expresiones lógicas complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptron es una de las arquitecturas RNA más simples, inventada en 1957 por Frank Rosenblatt. Se basa en una neurona artificial ligeramente diferente (ver la siguiente figura) llamada *unidad lógica de umbral* (*threshold logic unit - TLU*) o algunas veces *unidad de umbral lineal* (*linear threshold unit - LTU*): las entradas y las salidas son ahora números (en lugar de valores binarios on/off) y cada conexión de entrada está asociada con un peso. La TLU calcula una suma ponderada de sus entradas ($z = w_1x_1 + w_2x_2 + \\dots + w_nx_n = x^Tw$) y luego aplica una *función de paso* a esta suma y devuelve el resultado: $h_w(x) = \\text{step}(z)$, donde $z = x^Tw$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TLU](images/ch10/TLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de paso más común usada en los perceptrones es la *función Heaviside*. Algunas veces se usa la función señal en su lugar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![heaviside](images/ch10/heaviside.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una única TLU se puede usar para clasificación binaria lineal simple. Calcula una combinación lineal de las entradas y si el resultado excede un umbral, devuelve la clase positica y si no devuelve la clase negativa (como un clasificador de regresión logística o un SVM lineal)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitddc70b56f9d74bcebb5cbcc975b589c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
