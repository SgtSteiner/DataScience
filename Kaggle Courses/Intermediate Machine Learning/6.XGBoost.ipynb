{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Automático - Intermedio\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "### Introducción\n",
    "\n",
    "En este tutorial, aprenderemos a construir y optimizar modelos con **gradient boosting**. Este método domina muchas competiciones de Kaggle y logra resultados de vanguardia en una variedad de conjuntos de datos.\n",
    "\n",
    "Durante gran parte de este curso hemos realizado predicciones con el método de random forest, que logra un mejor rendimiento que un solo árbol de decisión simplemente promediando las predicciones de muchos árboles de decisión.\n",
    "\n",
    "Nos referimos al método de random forest como un \"método de conjunto\". Por definición, los **métodos de conjunto** combinan las predicciones de varios modelos (por ejemplo, varios árboles, en el caso de random forest).\n",
    "\n",
    "A continuación, aprenderemos sobre otro método de conjunto llamado gradient boosting.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "**Gradient boosting** es un método que pasa por ciclos para agregar modelos de forma iterativa a un conjunto.\n",
    "\n",
    "Comienza por inicializar el conjunto con un solo modelo, cuyas predicciones pueden ser bastante ingenuas. (Incluso si sus predicciones son muy inexactas, las adiciones posteriores al conjunto abordarán esos errores).\n",
    "\n",
    "Entonces, comenzamos el ciclo:\n",
    "\n",
    "+ Primero, usamos el conjunto actual para generar predicciones para cada observación en el conjunto de datos. Para hacer una predicción, agregamos las predicciones de todos los modelos del conjunto.\n",
    "+ Estas predicciones se utilizan para calcular una función de pérdida (como el [error cuadrático medio](https://en.wikipedia.org/wiki/Mean_squared_error), por ejemplo).\n",
    "+ Luego, usamos la función de pérdida para ajustar un nuevo modelo que se agregará al conjunto. Específicamente, determinamos los parámetros del modelo para que agregar este nuevo modelo al conjunto reduzca la pérdida. (Nota al margen: *el \"gradient\" en \"gradient boosting\" se refiere al hecho de que usaremos el [descenso de gradiente](https://en.wikipedia.org/wiki/Gradient_descent) en la función de pérdida para determinar los parámetros en este nuevo modelo*).\n",
    "+ Finalmente, agregamos el nuevo modelo al conjunto y ...\n",
    "+ ... ¡repetir!\n",
    "\n",
    "![gradient_boosting](./images/gradient_boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Comenzamos cargando los datos de entrenamiento y validación en `X_train`, `X_valid`, `y_train` e `y_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Leer datos\n",
    "data = pd.read_csv('./input/melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# Selecciona subconjunto de predictores\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# Selecciona objetivo\n",
    "y = data.Price\n",
    "\n",
    "# Separa los datos en conjutos de entrenamientos y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, trabajaremos con la biblioteca XGBoost. **XGBoost** significa **extreme gradient boosting**, que es una implementación de gradient boosting con varias características adicionales centradas en el rendimiento y la velocidad. (*Scikit-learn tiene otra versión de gradient boosting, pero XGBoost tiene algunas ventajas técnicas*).\n",
    "\n",
    "Importamos la API scikit-learn para XGBoost (`xgboost.XGBRegressor`). Esto nos permite construir y ajustar un modelo tal como lo haríamos en scikit-learn. Como verás en la salida, la clase `XGBRegressor` tiene muchos parámetros ajustables: ¡pronto los aprenderás!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "my_model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También hacemos predicciones y evaluamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de parámetros\n",
    "\n",
    "XGBoost tiene algunos parámetros que pueden afectar dramáticamente la precisión y la velocidad de entrenamiento. Los primeros parámetros que se deben comprender son:\n",
    "\n",
    "`n_estimators`\n",
    "\n",
    "`n_estimators` especifica cuántas veces pasar por el ciclo de modelado descrito anteriormente. Es igual al número de modelos que incluimos en el conjunto.\n",
    "\n",
    "+ Un valor demasiado *bajo* provoca *underfitting*, lo que conduce a predicciones inexactas tanto en los datos de entrenamiento como en los de prueba.\n",
    "+ Un valor demasiado *alto* provoca un *overfitting*, lo que genera predicciones precisas sobre los datos de entrenamiento, pero predicciones inexactas sobre los datos de las pruebas (que es lo que nos importa).\n",
    "\n",
    "Los valores típicos oscilan entre 100 y 1000, aunque esto depende mucho del parámetro `learning_rate` que se analiza a continuación.\n",
    "\n",
    "Aquí está el código para establecer el número de modelos en el conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`early_stopping_rounds`\n",
    "`early_stopping_rounds` ofrece una manera de encontrar automáticamente el valor ideal para `n_estimators`. *Early stopping* hace que el modelo deje de iterar cuando la puntuación de validación deja de mejorar, incluso si no estamos en la parada indicada para `n_estimators`. Es inteligente establecer un valor alto para `n_estimators` y luego usar `early_stopping_rounds` para encontrar el momento óptimo para dejar de iterar.\n",
    "\n",
    "Dado que la probabilidad aleatoria a veces causa una sola ronda en la que los puntajes de validación no mejoran, se debe especificar un número para cuántas rondas de deterioro directo se permiten antes de detenerse. Establecer `early_stopping_rounds = 5` es una opción razonable. En este caso, nos detenemos después de 5 rondas consecutivas de puntuaciones de validación deteriorados.\n",
    "\n",
    "Al usar `early_stopping_rounds`, también se deben reservar algunos datos para calcular las puntuaciones de validación; esto se hace configurando el parámetro `eval_set`.\n",
    "\n",
    "Podemos modificar el ejemplo anterior para incluir la early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)],\n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si más tarde se desea ajustar un modelo con todos sus datos establezca `n_estimators` en el valor que considere óptimo cuando se ejecuta con detención temprana.\n",
    "\n",
    "`learning rate`\n",
    "\n",
    "En lugar de obtener predicciones simplemente sumando las predicciones de cada modelo de componente, podemos multiplicar las predicciones de cada modelo por un pequeño número (conocido como la tasa de aprendizaje p **learning rate**) antes de sumarlas.\n",
    "\n",
    "Esto significa que cada árbol que agreguemos al conjunto nos ayuda menos. Por lo tanto, podemos establecer un valor más alto para `n_estimators` sin overfitting. Si utilizamos early stopping, la cantidad apropiada de árboles se determinará automáticamente.\n",
    "\n",
    "En general, una pequeña tasa de aprendizaje y un gran número de estimadores producirán modelos XGBoost más precisos, aunque también llevará más tiempo entrenar el modelo, ya que realiza más iteraciones a lo largo del ciclo. De forma predeterminada, XGBoost establece `learning_rate = 0.1`.\n",
    "\n",
    "Al modificar el ejemplo anterior para cambiar la tasa de aprendizaje, se obtiene el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_jobs`\n",
    "\n",
    "En conjuntos de datos más grandes donde el tiempo de ejecución es una consideración, se puede usar el paralelismo para construir nuestros modelos más rápido. Es común establecer el parámetro `n_jobs` igual al número de núcleos en su máquina. En conjuntos de datos más pequeños, esto no ayudará.\n",
    "\n",
    "El modelo resultante no será mejor, por lo que la micro-optimización para el tiempo de adaptación generalmente no es más que una distracción. Pero es útil en grandes conjuntos de datos en los que, de lo contrario, pasaría mucho tiempo esperando durante el comando `fit`.\n",
    "\n",
    "Aquí está el ejemplo modificado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/latest/) es una librería de software líder para trabajar con datos tabulares estándar (el tipo de datos que se almacena en Pandas DataFrames, en oposición a tipos de datos más exóticos como imágenes y videos). Con un ajuste cuidadoso de los parámetros, se pueden entrenar modelos de alta precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
