{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Prediction - Part 1 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.pixabay.com/photo/2016/03/09/11/53/wine-glasses-1246240_1280.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a trilogy in which I will approach the wine quality dataset from several different approaches:\n",
    "\n",
    "+ [Part 1: Supervised Learning - Regression](https://www.kaggle.com/sgtsteiner/red-wine-quality-regression)\n",
    "+ [Part 2: Supervised Learning - Multiclass Classification](https://www.kaggle.com/sgtsteiner/red-wine-quality-multiclass-classification)\n",
    "+ [Part 3: Supervised Learning - Binary Classification](https://www.kaggle.com/sgtsteiner/red-wine-quality-binary-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset that contains various characteristics of red and white variants of the Portuguese \"Vinho Verde\" wine. We have chemical variables, such as the amount of alcohol, citric acid, acidity, density, pH, etc; as well as a sensorial and subjective variable such as the score with which a group of experts rated the quality of the wine: between 0 (very bad) and 10 (very excellent).\n",
    "\n",
    "They ask us to build a model that can predict the quality score given these biochemical indicators.\n",
    "\n",
    "For this first part of the study, we are going to consider that it is a **regression problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, ElasticNet, Ridge\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the size and type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Type\": red.dtypes,\n",
    "              \"Unique\": red.nunique(),\n",
    "              \"Null\": red.isnull().sum(),\n",
    "              \"Null percent\": red.isnull().sum() / len(red),\n",
    "              \"Mean\": red.mean(),\n",
    "              \"Std\": red.std()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmmmm, there are no nulls, what a data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the features distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.hist(bins=50, figsize=(15,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our target variable, the quality score, is distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of quality scores\")\n",
    "red[\"quality\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is significantly unbalanced. Most instances (82%) have scores of 6 or 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check the correlations between the attributes of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = red.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(red.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show only the correlations of the target variable with the rest of the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "corr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title(\"Attribute correlations with quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the predictor set and the set with the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_columns = red.columns[:-1]\n",
    "predict_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = red[predict_columns]\n",
    "y = red[\"quality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortlist Promising Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we're going train several quick-and-dirty models from different categories using standard parameters. We selected some of the regression models: Linear Regression, Lasso, ElasticNet, Ridge, Extre Trees, and RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(estimator, X_train, y_train, cv=10, verbose=True):\n",
    "    \"\"\"Print and return cross validation of model\n",
    "    \"\"\"\n",
    "    scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"]\n",
    "    scores = cross_validate(estimator, X_train, y_train, return_train_score=True, cv=cv, scoring=scoring)\n",
    "    \n",
    "    val_mae_mean, val_mae_std = -scores['test_neg_mean_absolute_error'].mean(), \\\n",
    "                                -scores['test_neg_mean_absolute_error'].std()\n",
    "    \n",
    "    train_mae_mean, train_mae_std = -scores['train_neg_mean_absolute_error'].mean(), \\\n",
    "                                    -scores['train_neg_mean_absolute_error'].std()\n",
    "    \n",
    "    val_mse_mean, val_mse_std = -scores['test_neg_mean_squared_error'].mean(), \\\n",
    "                                -scores['test_neg_mean_squared_error'].std()\n",
    "    \n",
    "    train_mse_mean, train_mse_std = -scores['train_neg_mean_squared_error'].mean(), \\\n",
    "                                    -scores['train_neg_mean_squared_error'].std()\n",
    "    \n",
    "    val_rmse_mean, val_rmse_std = np.sqrt(-scores['test_neg_mean_squared_error']).mean(), \\\n",
    "                                  np.sqrt(-scores['test_neg_mean_squared_error']).std()\n",
    "    \n",
    "    train_rmse_mean, train_rmse_std = np.sqrt(-scores['train_neg_mean_squared_error']).mean(), \\\n",
    "                                      np.sqrt(-scores['train_neg_mean_squared_error']).std()\n",
    "    \n",
    "    val_r2_mean, val_r2_std = scores['test_r2'].mean(), scores['test_r2'].std()\n",
    "    \n",
    "    train_r2_mean, train_r2_std = scores['train_r2'].mean(), scores['train_r2'].std()\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        \"Val MAE\": val_mae_mean,\n",
    "        \"Val MAE std\": val_mae_std,\n",
    "        \"Train MAE\": train_mae_mean,\n",
    "        \"Train MAE std\": train_mae_std,\n",
    "        \"Val MSE\": val_mse_mean,\n",
    "        \"Val MSE std\": val_mse_std,\n",
    "        \"Train MSE\": train_mse_mean,\n",
    "        \"Train MSE std\": train_mse_std,\n",
    "        \"Val RMSE\": val_rmse_mean,\n",
    "        \"Val RMSE std\": val_rmse_std,\n",
    "        \"Train RMSE\": train_rmse_mean,\n",
    "        \"Train RMSE std\": train_rmse_std,\n",
    "        \"Val R2\": val_r2_mean,\n",
    "        \"Val R2 std\": val_r2_std,\n",
    "        \"Train R2\": train_rmse_mean,\n",
    "        \"Train R2 std\": train_r2_std,\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"val_MAE_mean: {val_mae_mean} - (std: {val_mae_std})\")\n",
    "        print(f\"train_MAE_mean: {train_mae_mean} - (std: {train_mae_std})\")\n",
    "        print(f\"val_MSE_mean: {val_mse_mean} - (std: {val_mse_std})\")\n",
    "        print(f\"train_MSE_mean: {train_mse_mean} - (std: {train_mse_std})\")\n",
    "        print(f\"val_RMSE_mean: {val_rmse_mean} - (std: {val_rmse_std})\")\n",
    "        print(f\"train_RMSE_mean: {train_rmse_mean} - (std: {train_rmse_std})\")\n",
    "        print(f\"val_R2_mean: {val_r2_mean} - (std: {val_r2_std})\")\n",
    "        print(f\"train_R2_mean: {train_r2_mean} - (std: {train_r2_std})\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression(), Lasso(alpha=0.1), ElasticNet(),\n",
    "          Ridge(), ExtraTreesRegressor(), RandomForestRegressor()]\n",
    "\n",
    "model_names = [\"Lineal Regression\", \"Lasso\", \"ElasticNet\",\n",
    "               \"Ridge\", \"Extra Tree\", \"Random Forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = []\n",
    "mse = []\n",
    "rmse = []\n",
    "r2 = []\n",
    "\n",
    "for model in range(len(models)):\n",
    "    print(f\"Paso {model+1} de {len(models)}\")\n",
    "    print(f\"...running {model_names[model]}\")\n",
    "    \n",
    "    rg_scores = evaluate_model(models[model], X_train, y_train)\n",
    "    \n",
    "    mae.append(rg_scores[\"Val MAE\"])\n",
    "    mse.append(rg_scores[\"Val MSE\"])\n",
    "    rmse.append(rg_scores[\"Val RMSE\"])\n",
    "    r2.append(rg_scores[\"Val R2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the performance of each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({\"Model\": model_names,\n",
    "                          \"MAE\": mae,\n",
    "                          \"MSE\": mse,\n",
    "                          \"RMSE\": rmse,\n",
    "                          \"R2\": r2})\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.sort_values(by=\"RMSE\", ascending=False).plot.barh(\"Model\", \"RMSE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.sort_values(by=\"R2\").plot.barh(\"Model\", \"R2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that gives the best results is **extra trees**. RMSE = 0.577591 and R2 = 0.477845. Let's fine tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': range(10, 300, 10), 'max_features': [2, 3, 4, 5, 8, \"auto\"], 'bootstrap': [True, False]}\n",
    "]\n",
    "\n",
    "\n",
    "xtree_reg = ExtraTreesRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(xtree_reg, param_grid, cv=5, \n",
    "                           scoring='neg_mean_squared_error', \n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the moment of truth! Let's see the performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "y_pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"MSE: {metrics.mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred))}\")\n",
    "print(f\"R2: {final_model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, a little better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.1)\n",
    "plt.xlabel(\"Real\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which features are most relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = final_model.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(feature_importances, X_test.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False)\n",
    "feature_imp.plot(kind='bar')\n",
    "plt.title('Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the errors are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resul = pd.DataFrame({\"Pred\": y_pred,\n",
    "              \"Real\": y_test,\n",
    "              \"error\": y_pred - y_test,\n",
    "              \"error_abs\": abs(y_pred - y_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resul[\"error\"].plot.hist(bins=40, density=True)\n",
    "plt.title(\"Error distribution\")\n",
    "plt.xlabel(\"Error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, What's the MAE that occurs in each quality score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resul.groupby(\"Real\")[\"error_abs\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resul.groupby(\"Real\")[\"error_abs\"].mean().plot.bar()\n",
    "plt.title(\"MAE distribution\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"Quality\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "After testing various models, the one that provided the best results is ExtraTrees. After fine tuning it, we get a significant improvement.\n",
    "\n",
    "The basic line regression model offers an R2: 0.323021 and RMSE: 0.657899. The Extra Tree model offers an R2: 0.529512 and RMSE: 0.570954. However, the R2 score is still very low. According to the value obtained from R2, our model can barely explain 52% of the variance. That is, the percentage of relationship between the variables that can be explained by our model is 52.95%.\n",
    "\n",
    "According to the MAE distribution graph, we can see that our model is not good for extreme scores. In fact, it is not capable of predicting any score of 3 or 8. As we saw in the distribution of the target variable, it is very unbalanced, there are hardly any observations for the extreme values, so the model does not have enough data training for all quality scores.\n",
    "\n",
    "As a final consideration, we should try to approach modeling as a classification problem, to evaluate if it offers better results than a regression problem. We will see it in part 2 and 3 of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
