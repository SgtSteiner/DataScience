{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Aprendizaje Automático.\n",
    "\n",
    "## Subajute y sobreasjute\n",
    "\n",
    "### Experimentando con diferentes modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final de esta lección, comprenderemos los conceptos de subajuste y sobreajuste (*underfitting* y *overfitting*) y podremos aplicar estas ideas para que nuestros modelos sean más precisos.\n",
    "\n",
    "Ahora que disponemos de una forma confiable de medir la precisión del modelo, podemos experimentar con modelos alternativos y ver cuál ofrece las mejores predicciones. Pero, ¿qué alternativas tenemos para los modelos?\n",
    "\n",
    "Podemos ver en la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) de scikit-learn que el modelo de árbol de decisión tiene muchas opciones (más de lo que querremos o necesitaremos durante mucho tiempo). Las opciones más importantes determinan la profundidad del árbol. Recuerdemos que la profundidad de un árbol es una medida de cuántas divisiones hace antes de llegar a una predicción. Este es un árbol relativamente poco profundo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Arbol profundo](./images/arbol_profundo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica, no es raro que un árbol tenga 10 divisiones entre el nivel superior (todas las casas) y una hoja. A medida que el árbol se hace más profundo, el conjunto de datos se divide en hojas con menos casas. Si un árbol solo tiene 1 división, divide los datos en 2 grupos. Si cada grupo se divide nuevamente, obtendríamos 4 grupos de casas. Dividir cada uno de esos nuevamente crearía 8 grupos. Si seguimos duplicando el número de grupos agregando más divisiones en cada nivel, tendremos 2<sup>10</sup> grupos de casas para cuando lleguemos al décimo nivel. Eso es 1024 hojas.\n",
    "\n",
    "Cuando dividimos las casas entre muchas hojas, también tenemos menos casas en cada hoja. Las hojas con muy pocas casas harán predicciones muy cercanas a los valores reales de esas casas, pero pueden hacer predicciones muy poco confiables para nuevos datos (porque cada predicción se basa en solo unas pocas casas).\n",
    "\n",
    "Este es un fenómeno llamado **sobreajuste** (*overfitting*), donde un modelo coincide con los datos de entrenamiento casi a la perfección, pero no funciona bien en la validación y otros datos nuevos. Por otro lado, si hacemos que nuestro árbol sea muy poco profundo, no divide las casas en grupos muy distintos.\n",
    "\n",
    "En un extremo, si un árbol divide las casas en solo 2 ó 4, cada grupo todavía tiene una gran variedad de casas. Las predicciones resultantes pueden estar muy lejos para la mayoría de las casas, incluso en los datos de entrenamiento (y también será malo en la validación por la misma razón). Cuando un modelo no logra capturar distinciones y patrones importantes en los datos, por lo tanto, se desempeña mal incluso en los datos de entrenamiento, lo que se denomina adaptación insuficiente o **subajuste** (*underfitting*).\n",
    "\n",
    "Dado que nos preocupamos por la precisión de los nuevos datos, que estimamos a partir de nuestros datos de validación, queremos encontrar el punto óptimo entre el ajuste insuficiente y el sobreajuste. Visualmente, queremos el punto bajo de la curva de validación (roja) en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Over_Underfitting](./images/over_underfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Existen algunas alternativas para controlar la profundidad del árbol y muchas permiten que algunas rutas a través del árbol tengan mayor profundidad que otras rutas. Pero el argumento *max_leaf_nodes* proporciona una forma muy sensata de controlar el overfitting vs underfitting. Cuantas más hojas permitimos que haga el modelo, más nos movemos desde el área de underfitting en el gráfico anterior al área de overfitting.\n",
    "\n",
    "Podemos usar una función de utilidad para ayudar a comparar las puntuaciones de MAE de diferentes valores para *max_leaf_nodes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Cargamos los datos\n",
    "melbourne_file_path = \"./input/melbourne-housing-snapshot/melb_data.csv\"\n",
    "melbourne_data = pd.read_csv(melbourne_file_path) \n",
    "# Eliminamos las filas con valores de precio ausentes\n",
    "filtered_melbourne_data = melbourne_data.dropna(axis=0)\n",
    "# Elegimos el objetivo y las características\n",
    "y = filtered_melbourne_data.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = filtered_melbourne_data[melbourne_features]\n",
    "\n",
    "# Divide los datos en datos de entrenamiento y validación, tanto para las características como para el objetivo\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar un bucle for para comparar la precisión de los modelos construidos con diferentes valores de *max_leaf_nodes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  347380\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  258171\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  243495\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  254983\n"
     ]
    }
   ],
   "source": [
    "# Compara el MAE con diferentes valores de max_leaf_nodes\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las opciones enumeradas, 500 es el número óptimo de hojas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las modelos pueden sufrir\n",
    "\n",
    "+ Overfitting: capturar patrones espurios que no se repetirán en el futuro, lo que conducirá a predicciones menos precisas, o\n",
    "+ Underfitting: no capturar patrones relevantes, nuevamente conduce a predicciones menos precisas.\n",
    "\n",
    "Utilizamos datos de **validación**, que no se utilizan en la capacitación de modelos, para medir la precisión de un modelo candidato. Esto nos permite probar muchos modelos candidatos y mantener el mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
