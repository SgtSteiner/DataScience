{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Aprendizaje Automático.\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles de decisión te dejan con una decisión difícil. Un árbol profundo con muchas hojas se sobreajustará porque cada predicción proviene de datos históricos de solo las pocas casas en su hoja. Pero un árbol poco profundo con pocas hojas funcionará mal porque no logra capturar tantas distinciones en los datos sin procesar.\n",
    "\n",
    "Incluso las técnicas de modelado más sofisticadas de hoy se enfrentan a esta tensión entre el underfitting y el overfitting. Pero muchos modelos tienen ideas inteligentes que pueden conducir a un mejor rendimiento. Veremos el bosque aleatorio (**random forest**) como un ejemplo.\n",
    "\n",
    "El bosque aleatorio usa muchos árboles y realiza una predicción promediando las predicciones de cada árbol componente. En general, tiene una precisión predictiva mucho mejor que un solo árbol de decisión y funciona bien con los parámetros predeterminados. Si sigues modelando puedes aprender más modelos con un rendimiento aún mejor, pero muchos de ellos son sensibles para obtener los parámetros correctos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Cargamos los datos\n",
    "melbourne_file_path = \"./input/melbourne-housing-snapshot/melb_data.csv\"\n",
    "melbourne_data = pd.read_csv(melbourne_file_path) \n",
    "# Eliminamos las filas con valores de precio ausentes\n",
    "filtered_melbourne_data = melbourne_data.dropna(axis=0)\n",
    "# Elegimos el objetivo y las características\n",
    "y = filtered_melbourne_data.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = filtered_melbourne_data[melbourne_features]\n",
    "\n",
    "# Divide los datos en datos de entrenamiento y validación, tanto para las características como para el objetivo\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos un modelo de bosque aleatorio similar a cómo construimos un árbol de decisión en scikit-learn, esta vez usando la clase `RandomForestRegressor` en lugar de `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202888.18157951365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Es probable que exista una mejora adicional, pero esta es una gran mejora con respecto al mejor error de árbol de decisión de 250.000. Hay parámetros que permiten cambiar el rendimiento del Bosque aleatorio tanto como cambiamos la profundidad máxima del árbol de decisión único. Pero una de las mejores características de los modelos de Random Forest es que generalmente funcionan de manera razonable incluso sin este ajuste.\n",
    "\n",
    "Pronto aprenderemos el modelo XGBoost, que proporciona un mejor rendimiento cuando se ajusta bien con los parámetros correctos (pero que requiere cierta habilidad para obtener los parámetros correctos del modelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validación MAE cuando no se especifica max_leaf_nodes: 29652.931506849316\n",
      "Validación MAE para el mejor valor de max_leaf_nodes: 27282.50803885739\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Path del archivo a leer\n",
    "iowa_file_path = \"./input/melbourne-housing-snapshot/train.csv\"\n",
    "\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n",
    "# Crea Objetivo\n",
    "y = home_data.SalePrice\n",
    "# Crea Características\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = home_data[features]\n",
    "\n",
    "# Divide loas datos en validación y entrenamiento\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Especifica Modelo\n",
    "iowa_model = DecisionTreeRegressor(random_state=1)\n",
    "# Entrena Modelo\n",
    "iowa_model.fit(train_X, train_y)\n",
    "\n",
    "# Hace predicciones y calcula el error medio absoluto\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validación MAE cuando no se especifica max_leaf_nodes: {}\".format(val_mae))\n",
    "\n",
    "# Using best value for max_leaf_nodes\n",
    "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
    "iowa_model.fit(train_X, train_y)\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validación MAE para el mejor valor de max_leaf_nodes: {}\".format(val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ciencia de datos no siempre es tan fácil. Pero reemplazar el árbol de decisión con un bosque aleatorio será una victoria fácil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1: Usar un Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validación MAE para el modelo Random Forest: 22762.42931506849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define el modelo. Establece random_state a 1\n",
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# Entrena el modelo\n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "# Calcula el error medio absoluto del modelo Random Forest con los datos de validación\n",
    "rf_val_predictions = rf_model.predict(val_X)\n",
    "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n",
    "\n",
    "print(\"Validación MAE para el modelo Random Forest: {}\".format(rf_val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos seguido instrucciones específicas en cada paso. Esto ayudó a aprender ideas clave y construir nuestro primer modelo, pero ahora sabemos lo suficiente como para probar las cosas por nuestra cuenta. Los concursos de Machine Learning son una excelente manera de probar nuestras propias ideas y aprender más mientras navegamos de manera independiente en un proyecto de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
